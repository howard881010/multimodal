{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'llm_chat' from 'src.vllm' (/data/kai/forecasting/multimodal/financial/notebooks/../src/vllm.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39msys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_chat, message_template\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'llm_chat' from 'src.vllm' (/data/kai/forecasting/multimodal/financial/notebooks/../src/vllm.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.sys.path.append(\"..\")\n",
    "from multimodal.financial.src.engine import llm_chat, message_template\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from glob import glob\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import ast\n",
    "\n",
    "# python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-70B-Instruct --tensor-parallel-size=2 --disable-log-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"1e2a803e-c6f0-4fb5-bfed-ce9ea9fa1539\"\n",
    "openai_api_base = \"https://llama3-70b-chat.nrp-nautilus.io/v1\"\n",
    "# model_name = \"llama3-70b\"\n",
    "# model = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "model = \"llama3-70b\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "def llm_chat(messages: list[dict], guided_json=None):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\n",
    "            \"guided_json\": guided_json\n",
    "        }\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ThreadPoolExecutor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm_chat(message_template(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrespond in 100 words\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mThreadPoolExecutor\u001b[49m(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m      6\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(test,) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m      7\u001b[0m     results \u001b[38;5;241m=\u001b[39m [future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ThreadPoolExecutor' is not defined"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    return llm_chat(message_template('respond in 100 words', 'hi'))\n",
    "\n",
    "workers = 100\n",
    "with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "    futures = [executor.submit(test,) for i in range(20)]\n",
    "    results = [future.result() for future in as_completed(futures)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit? I'm all ears!\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat about something in particular? I'm here to listen and assist if I can.\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat for a bit? I'm happy to talk about anything from your favorite hobbies to the latest news or trends. Or if you're feeling stuck, I can suggest some conversation topics. Just let me know what's on your mind!\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat about something in particular? I'm here to listen and assist if I can. If not, we could talk about a fun topic like a favorite hobby or a great book you've read recently. Let me know what's on your mind!\",\n",
       " \"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat about something in particular? I'm here to listen and assist if I can. If not, we could talk about a fun topic like your favorite hobby or a great book or movie you've enjoyed recently. Let me know what's on your mind!\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = {}\n",
    "    for k, v in data.items():\n",
    "        if type(v) == '' and v == '':\n",
    "            continue\n",
    "        elif type(v) == list and len(v) == 0:\n",
    "            continue\n",
    "        cleaned_data[k] = v\n",
    "    return cleaned_data\n",
    "\n",
    "def format_data_to_string(data):\n",
    "    result = \"\"\n",
    "    if data.get(\"share_price\", None) is not None:\n",
    "        result += \"Today's share price: \" + str(data[\"share_price\"])\n",
    "    for k, v in data.items():\n",
    "        if k != 'share_price':\n",
    "            result += f\"\\nToday's <{k}>: \" + str(v)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Given today's share price and stock related summary, predict the next day's share price and summary\"\n",
    "\n",
    "result_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"summary\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Summary of next day's article\"\n",
    "    },\n",
    "    \"share_price\": {\n",
    "        \"type\": \"integer\",\n",
    "        \"description\": \"Next day's share price prediction\"\n",
    "    }\n",
    "  },\n",
    "    \"required\": [\n",
    "    \"summary\",\n",
    "    \"share_price\",\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': \"Intel Corp's shares are expected to continue their downward trend as investors digest the company's weak forecast, with ongoing supply-chain disruptions and COVID-19 lockdowns in China likely to weigh on demand for PCs.\", 'share_price': 109}\n",
      "{'summary': \"AMD is expected to continue its strong growth momentum in Q1, driven by robust demand for processors and market share gains over Intel. However, the company faces challenges from supply chain issues, potential weakness in graphics card demand, and Intel's resurgence. Despite these headwinds, the stock is expected to rebound from its recent decline, driven by its strong fundamentals and growth prospects.\", 'share_price': 111}\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "paths = sorted(glob(\"/data/kai/forecasting/data/formatted_v0.2/AMD/*.json\"))\n",
    "def test_text_time(path):\n",
    "    timestamp = path.split('/')[-1].split(\".json\")[0]\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        cleaned_data = clean_data(data)\n",
    "\n",
    "    content = format_data_to_string(cleaned_data)\n",
    "    message = message_template(prompt, content)\n",
    "    response = llm_chat(message, guided_json=result_schema)\n",
    "    response = ast.literal_eval(response)\n",
    "\n",
    "    final_data = {}\n",
    "    final_data[\"timestamp\"] = timestamp\n",
    "    final_data[\"today_price\"] = cleaned_data.pop(\"share_price\")\n",
    "    final_data[\"predicted_price\"] = response[\"share_price\"]\n",
    "    final_data[\"today_summary\"] = cleaned_data\n",
    "    final_data[\"predicted_summary\"] = response[\"summary\"]\n",
    "\n",
    "    print(response)\n",
    "    return final_data\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = [executor.submit(test_text_time, path) for path in paths[:2]]\n",
    "    results = [future.result() for future in as_completed(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/data/kai/forecasting/data/predction_v0.2/text_time_text_time.json\", 'w') as f:\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/data/kai/forecasting/data/predction_v0.2/text_time_text_time.json\"\n",
    "with open(save_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x['timestamp'])\n",
    "\n",
    "# with open(save_path, 'w') as f:\n",
    "#     json.dump(results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
