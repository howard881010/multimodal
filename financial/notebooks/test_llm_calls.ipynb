{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(\"..\")\n",
    "from src.vllm import llm_chat, message_template\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from glob import glob\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import ast\n",
    "\n",
    "# python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-70B-Instruct --tensor-parallel-size=2 --disable-log-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "# model_name = \"llama3-70b\"\n",
    "model = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "if model == \"meta-llama/Meta-Llama-3-70B-Instruct\":\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "\n",
    "def llm_chat(messages: list[dict], guided_json=None):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        extra_body={\n",
    "            \"guided_json\": guided_json\n",
    "        }\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cleaned_data = {}\n",
    "    for k, v in data.items():\n",
    "        if type(v) == '' and v == '':\n",
    "            continue\n",
    "        elif type(v) == list and len(v) == 0:\n",
    "            continue\n",
    "        cleaned_data[k] = v\n",
    "    return cleaned_data\n",
    "\n",
    "def format_data_to_string(data):\n",
    "    result = \"\"\n",
    "    if data.get(\"share_price\", None) is not None:\n",
    "        result += \"Today's share price: \" + str(data[\"share_price\"])\n",
    "    for k, v in data.items():\n",
    "        if k != 'share_price':\n",
    "            result += f\"\\nToday's <{k}>: \" + str(v)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Given today's share price and stock related summary, predict the next day's share price and summary\"\n",
    "\n",
    "result_schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"summary\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Summary of next day's article\"\n",
    "    },\n",
    "    \"share_price\": {\n",
    "        \"type\": \"integer\",\n",
    "        \"description\": \"Next day's share price prediction\"\n",
    "    }\n",
    "  },\n",
    "    \"required\": [\n",
    "    \"summary\",\n",
    "    \"share_price\",\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': \"Intel Corp's shares are expected to continue their downward trend as investors digest the company's weak forecast, with ongoing supply-chain disruptions and COVID-19 lockdowns in China likely to weigh on demand for PCs.\", 'share_price': 109}\n",
      "{'summary': \"AMD is expected to continue its strong growth momentum in Q1, driven by robust demand for processors and market share gains over Intel. However, the company faces challenges from supply chain issues, potential weakness in graphics card demand, and Intel's resurgence. Despite these headwinds, the stock is expected to rebound from its recent decline, driven by its strong fundamentals and growth prospects.\", 'share_price': 111}\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "paths = sorted(glob(\"/data/kai/forecasting/data/formatted_v0.2/AMD/*.json\"))\n",
    "def test_text_time(path):\n",
    "    timestamp = path.split('/')[-1].split(\".json\")[0]\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        cleaned_data = clean_data(data)\n",
    "\n",
    "    content = format_data_to_string(cleaned_data)\n",
    "    message = message_template(prompt, content)\n",
    "    response = llm_chat(message, guided_json=result_schema)\n",
    "    response = ast.literal_eval(response)\n",
    "\n",
    "    final_data = {}\n",
    "    final_data[\"timestamp\"] = timestamp\n",
    "    final_data[\"today_price\"] = cleaned_data.pop(\"share_price\")\n",
    "    final_data[\"predicted_price\"] = response[\"share_price\"]\n",
    "    final_data[\"today_summary\"] = cleaned_data\n",
    "    final_data[\"predicted_summary\"] = response[\"summary\"]\n",
    "\n",
    "    print(response)\n",
    "    return final_data\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = [executor.submit(test_text_time, path) for path in paths[:2]]\n",
    "    results = [future.result() for future in as_completed(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/kai/forecasting/data/predction_v0.2/text_time_text_time.json\", 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
