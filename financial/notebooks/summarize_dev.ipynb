{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('../')\n",
    "from glob import glob\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from threading import Thread, Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from queue import Queue\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from src.vllm import batch_call_llm_chat, llm_chat, message_template\n",
    "from src.utils import load_json, chunk_documents\n",
    "import outlines\n",
    "from outlines.integrations.vllm import JSONLogitsProcessor, RegexLogitsProcessor\n",
    "from openai import OpenAI\n",
    "import threading\n",
    "import queue\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import concurrent.futures\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "ticker_paths = sorted(glob(\"/data/kai/forecasting/data/raw_v0.2/*.csv\"))\n",
    "summary_dir = \"/data/kai/forecasting/data/summary_v0.2\"\n",
    "document_dir = \"/data/kai/forecasting/data/document_v0.2\"\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "os.makedirs(document_dir, exist_ok=True)\n",
    "\n",
    "ticker = ticker_paths[0].split(\"/\")[-1].split('.csv')[0]\n",
    "df = pd.read_csv(ticker_paths[0])\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(prompt, doc, guided_json=None):\n",
    "    messages = message_template(prompt, doc)\n",
    "    response = llm_chat(messages, guided_json=guided_json)\n",
    "    data = json.loads(response)\n",
    "\n",
    "    return data\n",
    "\n",
    "def combine_results(results):\n",
    "    results_dict = {\n",
    "        'key_numbers': [],\n",
    "        'growth_trends': [],\n",
    "        'overall_market_outlook': [],\n",
    "        'major_stock_movements': [],\n",
    "        'significant_economic_indicators': [],\n",
    "        'notable_company_specific_news': [],\n",
    "        'summary': []\n",
    "    }\n",
    "\n",
    "    # Append each value to the appropriate list in results_dict\n",
    "    for result in results:\n",
    "        for key in result:\n",
    "            if isinstance(result[key], list):  # For lists of dictionaries (like 'key_numbers')\n",
    "                results_dict[key].extend(result[key])\n",
    "            else:\n",
    "                results_dict[key].append(result[key])\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case\n",
    "prompt = \"\"\"You are a helpful assistant for converting raw text of a\\\n",
    "stock news website into relevant text information. Summarize the following raw text by following the guideline:\n",
    "1. Filter out irrelevant information unrelated to stock news.\n",
    "2. Provide a concise summary that includes key numbers, growth trends, and the overall market outlook.\n",
    "3. Mention major stock movements, significant economic indicators, and any notable company-specific news.\n",
    "4. Avoid making up any information.\n",
    "\"\"\"\n",
    "# Test Case\n",
    "SUMMARY_PROMPT = \"\"\"You are a helpful assistant for converting raw text of a stock news website into relevant text information.\n",
    "1. Include key_numbers, growth_trends, overall_market_outlook, major_stock_movements, macroeconomic_numbers_or_trends, notable_company_specific_news, and a final summary.\n",
    "2. Preserve as much information as you can by always adding relevant units or details.\n",
    "3. Avoid making up any information.\n",
    "\"\"\"\n",
    "COMBINE_JSON_PROMPT = \"\"\"\n",
    "Combine the list of json into one json format.\n",
    "\"\"\"\n",
    "\n",
    "blocked_words = [\n",
    "    \"thestreet.comPlease enable JS and disable any ad blocker\",\n",
    "    \"wsj.comPlease enable JS and disable any ad blocker\",\n",
    "    \"Access Denied Access Denied You don't have permission to access\",\n",
    "    \"Access to this page has been denied\",\n",
    "    \"Sorry! Temporarily Unavailable Sorry, this page is temporarily unavailable for technical reasons.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ticker</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>https://www.aljazeera.com/economy/2022/3/1/us-...</td>\n",
       "      <td>A surge in oil sent shivers through risky asse...</td>\n",
       "      <td>-0.24775</td>\n",
       "      <td>Somewhat-Bearish</td>\n",
       "      <td>US stocks fall, oil tops $105 as Ukraine crisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp ticker                                                url  \\\n",
       "0  2022-03-01   AAPL  https://www.aljazeera.com/economy/2022/3/1/us-...   \n",
       "\n",
       "                                             summary  sentiment_score  \\\n",
       "0  A surge in oil sent shivers through risky asse...         -0.24775   \n",
       "\n",
       "    sentiment_label                                               text  \n",
       "0  Somewhat-Bearish  US stocks fall, oil tops $105 as Ukraine crisi...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = os.path.join(document_dir, f\"{ticker}.csv\")\n",
    "document_df = pd.DataFrame(columns=[\"row_idx\", \"doc_idx\", \"timestamp\", \"summary\"])\n",
    "document_df.to_csv(document_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.09it/s]\n"
     ]
    }
   ],
   "source": [
    "guided_json = load_json('../templates/guided_json_summary_v0.3.json')\n",
    "\n",
    "document_queue = Queue(maxsize=10)\n",
    "document_lock = Lock()\n",
    "\n",
    "def worker(queue):\n",
    "    while True:\n",
    "\n",
    "        data = queue.get()\n",
    "        if data is None:\n",
    "            break\n",
    "        row_idx, doc_idx, timestamp, document = data\n",
    "\n",
    "        with document_lock:\n",
    "            document_df = pd.read_csv(document_path)\n",
    "\n",
    "        if len(document_df[(document_df['row_idx'] == row_idx) & \\\n",
    "                        (document_df['doc_idx'] == doc_idx) &\\\n",
    "                        (document_df['timestamp'] == timestamp)]) == 0:\n",
    "            result = process_document(SUMMARY_PROMPT, document, guided_json)\n",
    "            new_row = pd.DataFrame({\"row_idx\": [row_idx], \"doc_idx\": [doc_idx], \"timestamp\": timestamp, \"summary\": [str(result)]})\n",
    "            with document_lock:\n",
    "                document_df = pd.read_csv(document_path)\n",
    "                document_df = pd.concat([document_df, new_row], ignore_index=True)\n",
    "                document_df.to_csv(document_path, index=False)\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "doc_workers = 4\n",
    "doc_threads = []\n",
    "for i in range(doc_workers):\n",
    "    thread = Thread(target=worker, args=(document_queue,))\n",
    "    thread.start()\n",
    "    doc_threads.append(thread)\n",
    "\n",
    "# Add documents to the queue\n",
    "for row_idx, row in tqdm(temp_df.iterrows(), total=temp_df.shape[0]):\n",
    "    if any(p.lower() in row['text'].strip().lower() for p in blocked_words):\n",
    "        continue\n",
    "\n",
    "    documents = chunk_documents(tokenizer, row['text'], 2048, overlap=512)\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        document_queue.put(( row_idx, doc_idx, row['timestamp'], doc))\n",
    "\n",
    "document_queue.join()\n",
    "\n",
    "# Stop the worker threads\n",
    "for i in range(doc_workers):\n",
    "    document_queue.put(None)\n",
    "for thread in doc_threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = combine_results(document_summaries[0])\n",
    "final_summary = process_document(COMBINE_JSON_PROMPT, str(result_dict), guided_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
