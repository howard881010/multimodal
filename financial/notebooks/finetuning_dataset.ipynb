{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datasets import Dataset\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.sys.path.append('/data/kai/forecasting/multimodal/financial')\n",
    "\n",
    "from templates.PROMPTS import ForecstBaselinePrompts\n",
    "from src.vllm import llm_chat\n",
    "\n",
    "from queue import Queue\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from threading import Thread\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "window = 5\n",
    "data_dir = '/data/kai/forecasting/data'\n",
    "formatted_paths = sorted(glob(os.path.join(data_dir, 'formatted') + \"/*\"))\n",
    "\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "def tokenize_dialog(dialog, tokenizer):\n",
    "    if tokenizer.vocab_size >= 128000:\n",
    "        dialog_tokens = tokenizer.apply_chat_template(dialog)\n",
    "        dialog_tokens = dialog_tokens[:-4] # Remove generation prompt <|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
    "        eot_indices = [i for i,n in enumerate(dialog_tokens) if n == 128009]\n",
    "        labels = copy.copy(dialog_tokens)\n",
    "        last_idx = 0\n",
    "        for n, idx in enumerate(eot_indices):\n",
    "            if n % 2 == 1:\n",
    "                last_idx = idx\n",
    "            else:\n",
    "                labels[last_idx:idx+1] = [-100] * (idx-last_idx+1)\n",
    "\n",
    "        dialog_tokens = [dialog_tokens]\n",
    "        labels_tokens = [labels]\n",
    "    else:\n",
    "        prompt_tokens = [tokenizer.encode(f\"{tokenizer.bos_token}{B_INST} {(prompt['content']).strip()} {E_INST}\", add_special_tokens=False) for prompt in dialog[::2]]\n",
    "        answer_tokens = [tokenizer.encode(f\"{answer['content'].strip()} {tokenizer.eos_token}\", add_special_tokens=False) for answer in dialog[1::2]]\n",
    "        dialog_tokens = list(itertools.chain.from_iterable(zip(prompt_tokens, answer_tokens)))\n",
    "\n",
    "        #Add labels, convert prompt token to -100 in order to ignore in loss function\n",
    "        labels_tokens = [len(c)*[-100,] if i % 2 == 0 else c for i,c in enumerate(dialog_tokens)]\n",
    "\n",
    "    combined_tokens = {\n",
    "        \"input_ids\": list(itertools.chain(*(t for t in dialog_tokens))),\n",
    "        \"labels\": list(itertools.chain(*(t for t in labels_tokens))),\n",
    "    }\n",
    "\n",
    "    return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))\n",
    "\n",
    "\n",
    "def get_financial_dataset():\n",
    "    def get_dataset():\n",
    "        for ticker_path in formatted_paths:\n",
    "            # initialize ticker and prompts\n",
    "            ticker = ticker_path.split('/')[-1].split('.csv')[0]\n",
    "            ticker_df = pd.read_csv(ticker_path)\n",
    "            prompts = ForecstBaselinePrompts(window)\n",
    "            \n",
    "            for i in range(0, len(ticker_df)-window*2):  # use next 5 price as ground truth\n",
    "                window_df = ticker_df.iloc[i: i+window*2]\n",
    "                window_prices = window_df['price'].values[:window]\n",
    "                window_summaries = window_df['summary'].values[:window]\n",
    "                x = []\n",
    "                for d, (price, summary) in enumerate(zip(window_prices, window_summaries)):\n",
    "                    x.append(f'<Day {d+1} Price>{price}, summary={summary}')\n",
    "                x = '<SEP>'.join(x)\n",
    "                y = ', '.join([str(price)\n",
    "                            for price in window_df['price'].values[window:window*2]])\n",
    "\n",
    "                messages = [{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": prompts.SYSTEM_PROMPT\n",
    "                },\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    'content': x\n",
    "                }]\n",
    "\n",
    "                yield tokenize_dialog(messages, tokenizer)\n",
    "\n",
    "    ds = Dataset.from_generator(get_dataset)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
