{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datasets\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.sys.path.append('/data/kai/forecasting/multimodal/financial')\n",
    "\n",
    "from templates.PROMPTS import ForecstBaselinePrompts\n",
    "from src.vllm import llm_chat\n",
    "\n",
    "from queue import Queue\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from threading import Thread\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "save_dir = '/data/kai/forecasting/results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "baseline_path = os.path.join(save_dir, f'{ticker}_baseline.csv')\n",
    "df = pd.read_csv(baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "      <th>Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184.21,181.95,179.69,178.43,176.17</td>\n",
       "      <td>190.29, 198.5, 195.21, 187.61, 186.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.15, 204.91, 207.67, 210.43, 213.19</td>\n",
       "      <td>187.61, 186.63, 192.03, 203.63, 207.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194.85, 197.41, 200.12, 202.95, 205.73</td>\n",
       "      <td>198.5, 195.21, 187.61, 186.63, 192.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203.12, 205.89, 208.75, 211.69, 214.63</td>\n",
       "      <td>195.21, 187.61, 186.63, 192.03, 203.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.11, 194.57, 197.03, 199.49, 201.95</td>\n",
       "      <td>186.63, 192.03, 203.63, 207.84, 216.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Result  \\\n",
       "0      184.21,181.95,179.69,178.43,176.17   \n",
       "1  202.15, 204.91, 207.67, 210.43, 213.19   \n",
       "2  194.85, 197.41, 200.12, 202.95, 205.73   \n",
       "3  203.12, 205.89, 208.75, 211.69, 214.63   \n",
       "4  192.11, 194.57, 197.03, 199.49, 201.95   \n",
       "\n",
       "                             Ground Truth  \n",
       "0   190.29, 198.5, 195.21, 187.61, 186.63  \n",
       "1  187.61, 186.63, 192.03, 203.63, 207.84  \n",
       "2   198.5, 195.21, 187.61, 186.63, 192.03  \n",
       "3  195.21, 187.61, 186.63, 192.03, 203.63  \n",
       "4  186.63, 192.03, 203.63, 207.84, 216.49  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "def tokenize_dialog(dialog, tokenizer):\n",
    "    if tokenizer.vocab_size >= 128000:\n",
    "        dialog_tokens = tokenizer.apply_chat_template(dialog)\n",
    "        dialog_tokens = dialog_tokens[:-4] # Remove generation prompt <|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
    "        eot_indices = [i for i,n in enumerate(dialog_tokens) if n == 128009]\n",
    "        labels = copy.copy(dialog_tokens)\n",
    "        last_idx = 0\n",
    "        for n, idx in enumerate(eot_indices):\n",
    "            if n % 2 == 1:\n",
    "                last_idx = idx\n",
    "            else:\n",
    "                labels[last_idx:idx+1] = [-100] * (idx-last_idx+1)\n",
    "\n",
    "        dialog_tokens = [dialog_tokens]\n",
    "        labels_tokens = [labels]\n",
    "    else:\n",
    "        prompt_tokens = [tokenizer.encode(f\"{tokenizer.bos_token}{B_INST} {(prompt['content']).strip()} {E_INST}\", add_special_tokens=False) for prompt in dialog[::2]]\n",
    "        answer_tokens = [tokenizer.encode(f\"{answer['content'].strip()} {tokenizer.eos_token}\", add_special_tokens=False) for answer in dialog[1::2]]\n",
    "        dialog_tokens = list(itertools.chain.from_iterable(zip(prompt_tokens, answer_tokens)))\n",
    "\n",
    "        #Add labels, convert prompt token to -100 in order to ignore in loss function\n",
    "        labels_tokens = [len(c)*[-100,] if i % 2 == 0 else c for i,c in enumerate(dialog_tokens)]\n",
    "\n",
    "    combined_tokens = {\n",
    "        \"input_ids\": list(itertools.chain(*(t for t in dialog_tokens))),\n",
    "        \"labels\": list(itertools.chain(*(t for t in labels_tokens))),\n",
    "    }\n",
    "\n",
    "    return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))\n",
    "\n",
    "\n",
    "def get_financial_dataset(dataset_config, tokenizer, split):\n",
    "    dataset = datasets.load_dataset(\"OpenAssistant/oasst1\", split=split)\n",
    "\n",
    "    dataset = dataset.map(lambda sample: {\n",
    "        \"message_id\": sample[\"message_id\"],\n",
    "        \"parent_id\": sample[\"parent_id\"],\n",
    "        \"text\": sample[\"text\"],\n",
    "        },\n",
    "        batched=True,\n",
    "        remove_columns=list(dataset.features),)\n",
    "\n",
    "    nodes = {}\n",
    "\n",
    "    messages = {}\n",
    "    root_ids = []\n",
    "\n",
    "    for data in dataset:\n",
    "        if data[\"parent_id\"]:\n",
    "            nodes[data[\"parent_id\"]] = nodes.get(data[\"parent_id\"], []) + [data[\"message_id\"]]\n",
    "        else:\n",
    "            root_ids.append(data[\"message_id\"])\n",
    "        messages[data[\"message_id\"]]=data[\"text\"]\n",
    "\n",
    "    def follow(thread, current_id):\n",
    "        thread = copy.copy(thread) + [messages[current_id]]\n",
    "        if current_id in nodes:\n",
    "            new_threads = []\n",
    "            for next_id in nodes[current_id]:\n",
    "                new_threads += follow(thread, next_id)\n",
    "            return new_threads\n",
    "        else:\n",
    "            return [thread]\n",
    "\n",
    "    def get_threads_from_root(root_id):\n",
    "        all_threads = []\n",
    "        thread = [messages[root_id]]\n",
    "        for cid in nodes[root_id]:\n",
    "            all_threads += follow(thread, cid)\n",
    "        return all_threads\n",
    "\n",
    "    dataset = dataset.filter(lambda x: x[\"message_id\"] in root_ids)\n",
    "    dataset = dataset.map(lambda x: {\"thread\": get_threads_from_root(x[\"message_id\"])}, remove_columns=list(dataset.features))\n",
    "    dataset = dataset.map(lambda x: {\"thread\": [i for row in x[\"thread\"] for i in row]}, batched=True)\n",
    "\n",
    "    def to_dialog(thread):\n",
    "        dialog = []\n",
    "        for i, content in enumerate(thread):\n",
    "            dialog.append({\n",
    "                \"role\": \"user\" if i % 2 == 0 else \"assistant\",\n",
    "                \"content\": content,\n",
    "            })\n",
    "        return {\"dialog\": dialog}\n",
    "\n",
    "    dataset = dataset.map(lambda x: to_dialog(x[\"thread\"]), remove_columns=list(dataset.features))\n",
    "    dataset = dataset.map(lambda x: tokenize_dialog(x[\"dialog\"], tokenizer), remove_columns=list(dataset.features))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 84437/84437 [00:01<00:00, 61143.86 examples/s]\n",
      "Filter: 100%|██████████| 84437/84437 [00:28<00:00, 2986.73 examples/s]\n",
      "Parameter 'function'=<function get_financial_dataset.<locals>.<lambda> at 0x7fecde4f4700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 9846/9846 [00:01<00:00, 7905.57 examples/s] \n",
      "Map: 100%|██████████| 9846/9846 [00:00<00:00, 9897.09 examples/s] \n",
      "Map: 100%|██████████| 44042/44042 [00:02<00:00, 20018.76 examples/s]\n",
      "Map: 100%|██████████| 44042/44042 [00:52<00:00, 843.84 examples/s] \n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = get_financial_dataset(None, tokenizer, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now explain it\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Financial dataset in following format\n",
    "# {\n",
    "#     '100', 'summary...',\n",
    "#     '105', 'summary...'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
