{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import NLinearModel\n",
    "from darts import TimeSeries\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput: x_time and x_text\\n\\n# LLM 2 ways\\nx_time: (\"110.23\") --> llm tokenizer --> time_embd_for_llm\\nx_text: input_id\\n\\n# NLinear\\nx_time --> n linear --> time_embd_for_nlinear\\nx_text --> bert or llm embd --> text_embd for nlinear\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "input: x_time and x_text\n",
    "\n",
    "# LLM 2 ways\n",
    "x_time: (\"110.23\") --> llm tokenizer --> time_embd_for_llm\n",
    "x_text: input_id\n",
    "\n",
    "# NLinear\n",
    "x_time --> n linear --> time_embd_for_nlinear\n",
    "x_text --> bert or llm embd --> text_embd for nlinear\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda:3\"\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, )\n",
    "\n",
    "# embeddings\n",
    "embd_dim = 4096\n",
    "window_size = 5\n",
    "embed_timeseries = nn.Linear(1, embd_dim)\n",
    "\n",
    "# Initialize weights and bias\n",
    "nn.init.normal_(embed_timeseries.weight, mean=0.0, std=0.01)\n",
    "nn.init.zeros_(embed_timeseries.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input time: 267 5 [111.98 108.41 102.95 105.53 106.46]\n",
      "Output time: 267 5 [104.29 102.25 109.33 115.37 111.69]\n",
      "Input text: 267 5 159\n",
      "Output text: 267 5 5\n"
     ]
    }
   ],
   "source": [
    "# run Upload_finance.ipynb\n",
    "# on path: \"/data/kai/forecasting/data/summary/summary_with_price_v0.2/AMD\"\n",
    "\n",
    "data_path = \"/data/kai/forecasting/data/test/AMD.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# n = df.shape[0]\n",
    "# train_n = int(n * 0.8)\n",
    "# train_df = df.iloc[:train_n]\n",
    "# valid_df = df.iloc[train_n:]\n",
    "\n",
    "def extract_inputs(df):\n",
    "    time_series = df[\"input\"].apply(lambda x: json.loads(x)[\"share_price\"]).values\n",
    "    input_time = np.array([time_series[i: i+window_size] for i in range(len(df) - window_size*2)])\n",
    "    output_time = np.array([time_series[i: i+window_size] for i in range(window_size, len(df) - window_size)])\n",
    "    # TODO: change to all instead of \"summary\"\n",
    "    input_text = df[\"input\"].apply(lambda x: json.loads(x)[\"summary\"]).values.tolist()\n",
    "    input_text = [input_text[i:i+window_size] for i in range(len(df) - window_size*2)]\n",
    "    output_text = [input_text[i:i+window_size] for i in range(window_size, len(df)-window_size)]\n",
    "    \n",
    "    # input_text: N x T x len(str)\n",
    "    return input_time, output_time, input_text, output_text\n",
    "\n",
    "input_time, output_time, input_text, output_text = extract_inputs(df)\n",
    "print(\"Input time:\", len(input_time), len(input_time[0]), input_time[0])\n",
    "print(\"Output time:\", len(output_time), len(output_time[0]), output_time[0])\n",
    "print(\"Input text:\", len(input_text), len(input_text[0]), len(input_text[0][0]))\n",
    "print(\"Output text:\", len(output_text), len(output_text[0]), len(output_text[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 4096]) torch.Size([1, 632, 4096]) torch.Size([1, 637, 4096])\n"
     ]
    }
   ],
   "source": [
    "# 1) Use linear embedding\n",
    "\n",
    "# B x window x 1\n",
    "time_input = torch.tensor(input_time[0]).unsqueeze(0).unsqueeze(-1).type(torch.float32)\n",
    "input_embeds_time = embed_timeseries(time_input).to(device)\n",
    "\n",
    "# hidden_states = inputs_embeds\n",
    "input_text_formatted = ','.join(input_text[0])\n",
    "embeddings = tokenizer(input_text_formatted, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "input_embeds_text = llm.base_model.embed_tokens(embeddings[:window_size].to(device))\n",
    "\n",
    "# concatenate\n",
    "llm_embed = torch.cat((input_embeds_time, input_embeds_text), dim=1)\n",
    "print(input_embeds_time.shape, input_embeds_text.shape, llm_embed.shape)\n",
    "\n",
    "# or add embedding for each token\n",
    "# tokenized[ day_1+time_embd_day_1, day_2+time_embd_day_1, day_3+time_embd_day_1...]\n",
    "\n",
    "# or add some exponential weighting\n",
    "# tokenized[ day_1+time_embd*[||___], day_2+time_embd_day_1*[__||___], day_3+time_embd_day_1*[____||]...]\n",
    "\n",
    "\n",
    "# how to modify llama embedding\n",
    "# transformers / models / llama / modeling_llama.py\n",
    "# Class LlamaModel: def forward\n",
    "# inputs_embeds\n",
    "# line 945: hidden_states = inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
