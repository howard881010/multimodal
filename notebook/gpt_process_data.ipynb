{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/multimodal/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluator.gpt_evaluator import FinancialDataProcessor\n",
    "import time\n",
    "import ast\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_completion(job_id, processor, poll_interval=50):\n",
    "    status = processor.check_status(job_id)\n",
    "    while status.status not in [\"completed\", \"failed\"]:\n",
    "        print(f\"Current status: {status}. Waiting for {poll_interval} seconds...\")\n",
    "        time.sleep(poll_interval)\n",
    "        status = processor.check_status(job_id)\n",
    "    return status.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name, input_dir, output_dir):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(os.path.join(input_dir, file_name))\n",
    "\n",
    "    # Step 1: Filter out rows where 'text' starts with \"Access to this page has been denied\"\n",
    "    df_filtered = df[~df['text'].str.startswith('Access to this page has been denied', na=False)].copy()\n",
    "\n",
    "    # Step 2: Ensure the timestamp column is in datetime format\n",
    "    df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
    "\n",
    "    # Step 3: Convert the timestamp to just the date part\n",
    "    df_filtered['date'] = df_filtered['timestamp'].dt.date\n",
    "\n",
    "    # Step 4: Define URLs to match MarketWatch, Reuters, and Investors in order of priority\n",
    "    url_priority = ['marketwatch', 'reuters', 'investors', 'benzinga', 'zacks', 'fool', 'thestreet', 'forbes']\n",
    "\n",
    "    # Step 5: Assign priority based on the URL. Check if the URL contains any of the specified keywords.\n",
    "    df_filtered['priority'] = df_filtered['url'].apply(lambda x: next((i for i, kw in enumerate(url_priority) if kw in x.lower()), len(url_priority)))\n",
    "\n",
    "    # Step 6: Sort by date and priority\n",
    "    df_filtered = df_filtered.sort_values(by=['date', 'priority'])\n",
    "\n",
    "    # Step 7: Group by date and choose up to 5 URLs (priority first, no duplicates per day)\n",
    "    def sample_top_five_per_day(group):\n",
    "        seen_sites = set()\n",
    "        sampled = pd.DataFrame()  # Initialize an empty DataFrame for sampled rows\n",
    "        \n",
    "        for idx, row in group.iterrows():\n",
    "            site = next((kw for kw in url_priority if kw in row['url'].lower()), None)\n",
    "            if site and site not in seen_sites:\n",
    "                sampled = pd.concat([sampled, group.loc[[idx]]])  # Append the row as DataFrame\n",
    "                seen_sites.add(site)\n",
    "            if len(sampled) >= 5:  # Stop once 5 distinct sites are found\n",
    "                break\n",
    "        \n",
    "        # If less than 5, fill with non-priority but ensure no duplicate sources\n",
    "        remaining_needed = 5 - len(sampled)\n",
    "        if remaining_needed > 0:\n",
    "            non_priority_urls = group[~group['url'].apply(lambda x: any(kw in x.lower() for kw in seen_sites))]\n",
    "            sampled_non_priority = non_priority_urls.head(remaining_needed)\n",
    "            sampled = pd.concat([sampled, sampled_non_priority])  # Append non-priority URLs\n",
    "\n",
    "        return sampled\n",
    "\n",
    "    sampled_data = df_filtered.groupby('date', group_keys=False).apply(sample_top_five_per_day)\n",
    "\n",
    "    # Step 8: Sort the data by date\n",
    "    sampled_data = sampled_data.sort_values(by='date')\n",
    "\n",
    "    # Step 9: Save the resulting DataFrame to a new file\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    sampled_data.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_results(output_dir, results_dir, file_name):\n",
    "    financial_data_processor = FinancialDataProcessor()\n",
    "\n",
    "    data = pd.read_csv(os.path.join(output_dir, file_name))\n",
    "\n",
    "    jsonl_path = os.path.join(results_dir, \"batch.jsonl\")\n",
    "    output_path = os.path.join(results_dir, \"data.txt\")\n",
    "\n",
    "    batch_object_id = financial_data_processor.create_and_run_batch_job(data, jsonl_path, ticker_column=\"ticker\", date_column=\"timestamp\", input_text_column=\"text\")\n",
    "\n",
    "    job_status = wait_for_completion(batch_object_id, financial_data_processor)\n",
    "\n",
    "    print(\"Job status:\", job_status)\n",
    "    if job_status == \"completed\":\n",
    "        print(\"Batch job completed successfully!\")\n",
    "        gpt_outputs = financial_data_processor.check_status_and_parse(batch_object_id , output_path)\n",
    "        dict_list = [ast.literal_eval(item) for item in gpt_outputs]\n",
    "        df = pd.DataFrame(dict_list)\n",
    "        # df['text'] = df.apply(lambda x: {\"ticker\": x['ticker'], \"summary\": x['summary']}, axis=1)\n",
    "        # df = df.drop(columns=['summary', 'keywords'])\n",
    "        df.to_csv(os.path.join(results_dir, file_name), index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_result(result_dir, combined_dir, file_name):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(os.path.join(result_dir, file_name))\n",
    "    \n",
    "    # Drop rows where 'summary' or 'keywords' are NaN\n",
    "    df_filtered = df.dropna(subset=['summary', 'keywords'])\n",
    "    \n",
    "    # Function to combine summaries and keywords\n",
    "    def combine_random(group):\n",
    "        # Randomly sample 3 or fewer summaries and keywords (if fewer than 3, choose what is available)\n",
    "        chosen_rows = group.sample(n=min(3, len(group)), random_state=1)\n",
    "        \n",
    "        # Combine the selected summaries\n",
    "        combined_summary = ' '.join(chosen_rows['summary'])\n",
    "        \n",
    "        # Combine keywords, split by commas, remove duplicates using a set, and then join them back\n",
    "        combined_keywords = ', '.join(sorted(set(', '.join(chosen_rows['keywords']).split(', '))))\n",
    "        \n",
    "        # Return a dictionary containing combined summary and keywords\n",
    "        return pd.Series({'text': combined_summary, 'keywords': combined_keywords})\n",
    "\n",
    "    # Group by 'ticker' and 'date', then apply the combine function\n",
    "    df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n",
    "\n",
    "    # Convert 'date' column in df_grouped to datetime\n",
    "    df_grouped['date'] = pd.to_datetime(df_grouped['date'])\n",
    "\n",
    "    # Extract the ticker from the file name, and find the date range for stock data\n",
    "    ticker = file_name.split(\".\")[0]\n",
    "    start_date = df_grouped['date'].min()\n",
    "    end_date = df_grouped['date'].max()\n",
    "    print(\"ticker: \", ticker, \"start_date: \", start_date, \"end_date: \", end_date)\n",
    "    \n",
    "    # Fetch the stock data for the given date range\n",
    "    df_stock = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "    # Reset the index to make 'Date' a column and work with a copy to avoid SettingWithCopyWarning\n",
    "    df_stock = df_stock.reset_index()[['Date', 'Close']].copy()\n",
    "\n",
    "    # Rename 'Date' to 'date' to match with the existing DataFrame\n",
    "    df_stock.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "    # Merge the two DataFrames on 'date'\n",
    "    df_merged = pd.merge(df_grouped, df_stock, on='date', how='left')\n",
    "\n",
    "    df_merged = df_merged.dropna(subset=['Close'])\n",
    "\n",
    "    # Display the merged DataFrame\n",
    "    print(\"Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\")\n",
    "    print(df_merged.head())\n",
    "\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    df_merged.to_csv(os.path.join(combined_dir, file_name), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518700/1724619003.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker:  AAPL start_date:  2022-03-01 00:00:00 end_date:  2024-04-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\n",
      "  ticker       date                                               text  \\\n",
      "0   AAPL 2022-03-01  Apple Inc. has halted product sales in Russia ...   \n",
      "1   AAPL 2022-03-02  Apple halted sales of its products in Russia f...   \n",
      "2   AAPL 2022-03-03  Apple stock climbed 2.1% to 166.56, moving tow...   \n",
      "3   AAPL 2022-03-04  In February, the U.S. stock market experienced...   \n",
      "6   AAPL 2022-03-07  Apple co-founder Steve Wozniak discussed Bitco...   \n",
      "\n",
      "                                            keywords       Close  \n",
      "0  Apple, Russia, S&P 500, Ukraine crisis, halt s...  163.199997  \n",
      "1  AAPL, Apple, Apple Pay, Russia, Ukraine, Ukrai...  166.559998  \n",
      "2  AAPL, Apple, Russia, Ukraine, Winslow Capital ...  166.229996  \n",
      "3  AAPL, Apple, Dow Jones, Russia-Ukraine, Russia...  163.169998  \n",
      "6  Apple, Beats, Bitcoin, HBCU, Living In Legacy,...  159.300003  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518700/1724619003.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker:  AMZN start_date:  2022-03-09 00:00:00 end_date:  2024-04-29 00:00:00\n",
      "Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\n",
      "  ticker       date                                               text  \\\n",
      "0   AMZN 2022-03-09  Amazon.com Inc. announced plans for a 20-to-1 ...   \n",
      "1   AMZN 2022-03-10  Amazon shares climbed more than 5% on March 10...   \n",
      "2   AMZN 2022-03-11  An Amazon gig deliveryman was shot multiple ti...   \n",
      "5   AMZN 2022-03-14  Amazon (AMZN) has a Zacks Rank of #3 (Hold) an...   \n",
      "6   AMZN 2022-03-15  Amazon plans a 20-for-1 stock split, making sh...   \n",
      "\n",
      "                                            keywords       Close  \n",
      "0  20-for-1, 20-to-1, 2022, 5.4% increase, AMZN, ...  139.279007  \n",
      "1  20-for-1, AMZN, Amazon, Bank of America, Dow J...  146.817505  \n",
      "2  2022, AMZN, Amazon, Chicago, March 11, Wall St...  145.524506  \n",
      "5  20-for-1 split, AMZN, Amazon, Blue Origin, DTC...  141.852997  \n",
      "6  $200 million investment, AWS, Amazon, Future R...  147.366501  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518700/1724619003.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker:  AMD start_date:  2022-03-04 00:00:00 end_date:  2024-04-28 00:00:00\n",
      "Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\n",
      "  ticker       date                                               text  \\\n",
      "0    AMD 2022-03-04  AMD is set to release its first-quarter earnin...   \n",
      "1    AMD 2022-03-07  Advanced Micro Devices (AMD) expects to releas...   \n",
      "2    AMD 2022-03-08  Advanced Micro Devices (AMD) has gained 31.1% ...   \n",
      "3    AMD 2022-03-09  Advanced Micro Devices (AMD) announced an addi...   \n",
      "4    AMD 2022-03-10  Shares of Advanced Micro Devices, Inc. (AMD) a...   \n",
      "\n",
      "                                            keywords       Close  \n",
      "0  AMD, CPU market share, Intel, console sales, e...  108.410004  \n",
      "1  AMD, ASML, Applied Materials, EPYC processors,...  102.949997  \n",
      "2  AMD, GPU, Infinity Cache, Mac Pro, RDNA 2, Rad...  105.529999  \n",
      "3  AMD, Advanced Micro Devices, Apple, Ryzen Thre...  111.050003  \n",
      "4  AMD, US Treasury yield, earnings, estimate, fi...  106.459999  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518700/1724619003.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker:  NFLX start_date:  2022-03-04 00:00:00 end_date:  2024-04-29 00:00:00\n",
      "Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\n",
      "  ticker       date                                               text  \\\n",
      "0   NFLX 2022-03-04  In April 2022, Netflix Inc. (NFLX) experienced...   \n",
      "3   NFLX 2022-03-07  Netflix has announced that it is suspending it...   \n",
      "4   NFLX 2022-03-08  Streaming leader Netflix Inc (NFLX) has had su...   \n",
      "5   NFLX 2022-03-09  Netflix (NFLX) gained 5% following an upgrade ...   \n",
      "6   NFLX 2022-03-10  The PGA Tour announced a Netflix docuseries wi...   \n",
      "\n",
      "                                            keywords       Close  \n",
      "0  April 2022, NFLX, Netflix, S&P 500, buy, decli...  361.730011  \n",
      "3  Netflix, Russia, Ukraine, conflict, invasion, ...  350.260010  \n",
      "4  Assassinâ€™s Creed, League of Legends, Netflix, ...  341.760010  \n",
      "5  NFLX, Netflix, Wedbush, bearish, bullish, earn...  358.790009  \n",
      "6  Box to Box Films, Formula 1: Drive to Survive,...  356.769989  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518700/1724619003.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_filtered.groupby(['ticker', 'date']).apply(combine_random).reset_index()\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker:  AVGO start_date:  2022-03-03 00:00:00 end_date:  2024-04-28 00:00:00\n",
      "Merged DataFrame with 'close' column added (rows with NaN 'close' dropped):\n",
      "  ticker       date                                               text  \\\n",
      "0   AVGO 2022-03-03  A whale has taken a bearish stance on Broadcom...   \n",
      "1   AVGO 2022-03-04  Broadcom Inc (AVGO) is a semiconductor giant s...   \n",
      "2   AVGO 2022-03-07  Shares of Broadcom Inc. (AVGO) gained 3% after...   \n",
      "3   AVGO 2022-03-08  Broadcom Inc. (AVGO) is a semiconductor and in...   \n",
      "4   AVGO 2022-03-09  Over the past three months, shares of Broadcom...   \n",
      "\n",
      "                                            keywords      Close  \n",
      "0  AVGO, Broadcom, Truist Securities, bearish sta...  57.860001  \n",
      "1  5G, AVGO, Broadcom, Nasdaq bear market, PCIe G...  59.598999  \n",
      "2  AVGO, Broadcom, Q1 fiscal 2022, Zacks Consensu...  57.070000  \n",
      "3  Broadcom, Infrastructure software, Semiconduct...  57.595001  \n",
      "4  AVGO, Broadcom, Electronics Semiconductors, VC...  59.702000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/ubuntu/multimodal/Data/financial-raw/\"\n",
    "output_dir = \"/home/ubuntu/multimodal/Data/financial-processed/\"\n",
    "results_dir = \"/home/ubuntu/multimodal/Data/financial-gpt/\"\n",
    "combined_dir = \"/home/ubuntu/multimodal/Data/financial-grouped\"\n",
    "\n",
    "file_names = [\"AAPL.csv\", \"AMZN.csv\", \"AMD.csv\", \"NFLX.csv\", \"AVGO.csv\"]\n",
    "# file_names = [\"AVGO.csv\"]\n",
    "# file_names = [\"CRM.csv\"]\n",
    "for file_name in file_names:\n",
    "    # process_data(file_name, input_dir, output_dir)\n",
    "    # get_gpt_results(output_dir, results_dir, file_name)\n",
    "    combined_result(results_dir, combined_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 2)\n",
      "Data grouped by date and saved to 'results_grouped_by_date.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Step 1: Retrieve all CSV file paths in the specified directory\n",
    "file_paths = glob.glob('/home/ubuntu/multimodal/Data/financial-gpt/*.csv')\n",
    "\n",
    "# Step 2: Read all CSV files and concatenate them into a single DataFrame\n",
    "all_data = pd.concat((pd.read_csv(file) for file in file_paths))\n",
    "\n",
    "# Step 3: Ensure the 'date' column is in datetime format for proper grouping\n",
    "all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "\n",
    "# Step 4: Replace NaN in the 'text' column with blank strings\n",
    "all_data['text'] = all_data['text'].fillna('')\n",
    "\n",
    "# Step 5: Group by the 'date' column and concatenate the 'text' column for each group\n",
    "grouped_data = all_data.groupby('date').agg({'text': '\\n'.join})\n",
    "\n",
    "# Step 6: Reset the index to flatten the DataFrame\n",
    "grouped_data = grouped_data.reset_index()\n",
    "\n",
    "# Step 7: Save the aggregated data to a CSV file\n",
    "grouped_data.to_csv('results_grouped_by_date.csv', index=False)\n",
    "\n",
    "# Print the shape of the resulting DataFrame\n",
    "print(grouped_data.shape)\n",
    "print(\"Data grouped by date and saved to 'results_grouped_by_date.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2022-03-22    168.820007\n",
      "2022-03-23    170.210007\n",
      "2022-03-24    174.070007\n",
      "2022-03-25    174.720001\n",
      "2022-03-28    175.600006\n",
      "                 ...    \n",
      "2024-03-19    176.080002\n",
      "2024-03-20    178.669998\n",
      "2024-03-21    171.369995\n",
      "2024-03-22    172.279999\n",
      "2024-03-25    170.850006\n",
      "Name: Close, Length: 505, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the stock ticker and the date range\n",
    "ticker = 'AAPL'\n",
    "start_date = '2022-03-22'\n",
    "end_date = '2024-03-26'\n",
    "\n",
    "# Fetch the historical data\n",
    "aapl_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Extract the 'Close' column\n",
    "aapl_close = aapl_data['Close']\n",
    "\n",
    "# Display the Close values\n",
    "print(aapl_close)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
