{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/multimodal/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluator.gpt_evaluator import FinancialDataProcessor\n",
    "import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_completion(job_id, processor, poll_interval=100):\n",
    "    status = processor.check_status(job_id)\n",
    "    while status.status not in [\"completed\", \"failed\"]:\n",
    "        print(f\"Current status: {status}. Waiting for {poll_interval} seconds...\")\n",
    "        time.sleep(poll_interval)\n",
    "        status = processor.check_status(job_id)\n",
    "    return status.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name, input_dir, output_dir):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(os.path.join(input_dir, file_name))\n",
    "\n",
    "    # Step 1: Filter out rows where 'text' starts with \"Access to this page has been denied\"\n",
    "    df_filtered = df[~df['text'].str.startswith('Access to this page has been denied', na=False)].copy()\n",
    "\n",
    "    # Step 2: Ensure the timestamp column is in datetime format\n",
    "    df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
    "\n",
    "    # Step 3: Convert the timestamp to just the date part\n",
    "    df_filtered['date'] = df_filtered['timestamp'].dt.date\n",
    "\n",
    "    # Step 4: Group by date and sample 5 entries from each group (or fewer if less than 5)\n",
    "    sampled_data = df_filtered.groupby('date', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), 5))\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Step 5: Sort the sampled data by date\n",
    "    sampled_data = sampled_data.sort_values(by='date')\n",
    "\n",
    "    # Step 6: Save the resulting DataFrame to a new file\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    sampled_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_results(output_dir, results_dir, file_name):\n",
    "    financial_data_processor = FinancialDataProcessor()\n",
    "\n",
    "    data = pd.read_csv(os.path.join(output_dir, file_name))\n",
    "\n",
    "    jsonl_path = os.path.join(results_dir, \"batch.jsonl\")\n",
    "    output_path = os.path.join(results_dir, \"data.txt\")\n",
    "\n",
    "    batch_object_id = financial_data_processor.create_and_run_batch_job(data, jsonl_path, ticker_column=\"ticker\", date_column=\"timestamp\", input_text_column=\"text\")\n",
    "\n",
    "    job_status = wait_for_completion(batch_object_id, financial_data_processor)\n",
    "\n",
    "    if job_status == \"completed\":\n",
    "        print(\"Batch job completed successfully!\")\n",
    "        gpt_outputs = financial_data_processor.check_status_and_parse(batch_object_id , output_path)\n",
    "        dict_list = [ast.literal_eval(item) for item in gpt_outputs]\n",
    "        df = pd.DataFrame(dict_list)\n",
    "        df_combined = df.groupby(['ticker', 'date'], as_index=False).agg({'text': ' '.join})\n",
    "        df_combined.to_csv(os.path.join(results_dir, file_name), index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_820258/3884984540.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = df_filtered.groupby('date', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job created with batch_object_id \n",
      " batch_CwflTdAa2qAlR0ZsqinUQ3PC\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=None, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=820, failed=0, total=3379)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=None, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=1613, failed=0, total=3379)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=None, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=2540, failed=0, total=3379)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='finalizing', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=1725331674, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=3378, failed=1, total=3379)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='finalizing', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=1725331674, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=3378, failed=1, total=3379)). Waiting for 100 seconds...\n",
      "Current status: Batch(id='batch_CwflTdAa2qAlR0ZsqinUQ3PC', completion_window='24h', created_at=1725331282, endpoint='/v1/chat/completions', input_file_id='file-bES3FFLJPfNqGPCoJNDeikPV', object='batch', status='finalizing', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725417682, failed_at=None, finalizing_at=1725331674, in_progress_at=1725331286, metadata={'description': 'Multimodal Forecasting'}, output_file_id=None, request_counts=BatchRequestCounts(completed=3378, failed=1, total=3379)). Waiting for 100 seconds...\n",
      "Batch job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/ubuntu/multimodal/Data/financial-raw/\"\n",
    "output_dir = \"/home/ubuntu/multimodal/Data/financial-processed/\"\n",
    "results_dir = f\"/home/ubuntu/multimodal/Data/financial-gpt/\"\n",
    "\n",
    "file_name = \"NFLX.csv\"\n",
    "process_data(file_name, input_dir, output_dir)\n",
    "get_gpt_results(output_dir, results_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 2)\n",
      "Data grouped by date and saved to 'results_grouped_by_date.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Step 1: Retrieve all CSV file paths in the specified directory\n",
    "file_paths = glob.glob('/home/ubuntu/multimodal/Data/financial-gpt/*.csv')\n",
    "\n",
    "# Step 2: Read all CSV files and concatenate them into a single DataFrame\n",
    "all_data = pd.concat((pd.read_csv(file) for file in file_paths))\n",
    "\n",
    "# Step 3: Ensure the 'date' column is in datetime format for proper grouping\n",
    "all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "all_data['text'] = all_data['text'].astype(str)\n",
    "\n",
    "# Step 4: Group by the 'date' column and concatenate the 'text' column for each group\n",
    "grouped_data = all_data.groupby('date').agg({'text': ' '.join})\n",
    "\n",
    "\n",
    "# Step 5: Reset the index to flatten the DataFrame\n",
    "grouped_data = grouped_data.reset_index()\n",
    "\n",
    "# Step 6: Save the aggregated data to a CSV file\n",
    "grouped_data.to_csv('results_grouped_by_date.csv', index=False)\n",
    "print(grouped_data.shape)\n",
    "\n",
    "print(\"Data grouped by date and saved to 'results_grouped_by_date.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
