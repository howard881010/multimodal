{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "from utils import split_data, convert_to_parquet, combine_window, combine_window_multiple_output\n",
    "import ast\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/numerical_2021-05-04_2023-12-04.csv\"\n",
    "# Path to the directory containing the CSV files\n",
    "climate_data = pd.read_csv(file_path)\n",
    "df = climate_data[['name', 'datetime', 'temp', 'longitude']]\n",
    "\n",
    "result = []\n",
    "# sort the state by longitude from left to right\n",
    "latitude_dict = df.set_index('name')['longitude'].to_dict()\n",
    "sorted_states = sorted(latitude_dict.keys(), key = lambda x: latitude_dict[x])\n",
    "\n",
    "# Group by 'datetime' and create the dictionary for 'temp' for each group\n",
    "for date, group in df.groupby('datetime'):\n",
    "    temp_dict = group.set_index('name')['temp'].to_dict()\n",
    "    \n",
    "    sorted_temp = {state: temp_dict[state] for state in sorted_states}\n",
    "    result.append({'datetime': date, 'temp': sorted_temp})\n",
    "    # only for california\n",
    "    # cal_temp = {\"California\": temp_dict['California']}\n",
    "    # result.append({'datetime': date, 'temp': cal_temp})\n",
    "    \n",
    "    \n",
    "\n",
    "# Create the final DataFrame\n",
    "new_df = pd.DataFrame(result)\n",
    "new_df.to_csv('/home/ubuntu/multimodal/Dataset/Climate-raw/procoess_num.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_of_week(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_of_week = date_obj.strftime('%A')\n",
    "    return f\"{date_str} ({day_of_week})\"\n",
    "\n",
    "def collapse_json(data):\n",
    "    collapsed_string = \"\"\n",
    "    data = ast.literal_eval(data)\n",
    "    for key, values in data.items():\n",
    "        collapsed_string += f\"{key}: \" + \" \".join(values) + \" \"\n",
    "    collapsed_string = collapsed_string.replace(\"_\", \" \")\n",
    "    return collapsed_string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/summarized_temperature_v1.csv\"\n",
    "# num_file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/temp_cal.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)   # from 05-01 - 12-01 \n",
    "# df_num = pd.read_csv(num_file_path)  # from 05-04 - 12-04\n",
    "\n",
    "# so the overall dataset is from 05-04 - 12-01\n",
    "\n",
    "df = df.iloc[:]\n",
    "# print(df['Time'].head(5))\n",
    "# print(df_num['datetime'].head(5))\n",
    "\n",
    "# df['temp'] = df_num['temp'].apply(ast.literal_eval).shift(3)\n",
    "df['temp'] = df['temp'].apply(lambda x: {\"DC\": x})\n",
    "df['summary'] = df['summary'].apply(collapse_json)\n",
    "df['Time'] = df['Time'].apply(add_day_of_week)\n",
    "df['summary_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary'], \"temperature\": x['temp']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary']}), axis=1)\n",
    "\n",
    "df['fut_summary'] = df['summary'].shift(-1)\n",
    "df['fut_summary_temp'] = df['summary_temp'].shift(-1)\n",
    "\n",
    "\n",
    "df_temp = df[['summary', 'fut_summary', 'summary_temp', 'fut_summary_temp']]\n",
    "df_temp.to_csv('/home/ubuntu/multimodal/Dataset/Climate/DC/temp-only/temp_dc.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/ubuntu/multimodal/Dataset/Climate/DC/temp-only/temp_dc.csv'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "split_data(file_path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d94307579bf41ad8052720d395a1bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b00bb3da770473d9dfd2533c61f230b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1daf1bacdc43d1810fe30073b1eefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ccbcfb7c7e4cfe9fc45c8488482ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c42c6412ce04015af022703ff55b83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158ad814d92c4179b8961f9aa9cc697d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526eda6bc60342f98b854490c4150da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f77aa5bd1a4d108b002fe94e04ad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70a3b64427e4d4db42fcbfc268ea10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bba88b4ced479394ed8ee23cf818ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce640b27641d46938637927d3c04262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a003f7b3234eaeba3d7844e6a1b081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8476287e47bb4cd287f0f3941fdc9136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5426bfd92a4b11b44184a1619094b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e66cd05c5204128885ff019ce213c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483338da4e7e43f79c5ec2df69836083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cbf5da695d47d6a7cdb03ea6cf089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38445f17fb7842d1b047d59f3f253d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02603b2b9174b0ba033750500cfdc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275699a8c2a0460999eccaacdeba34d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5bf716e5ef4c40921738c4e51d49d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/DC/temp-only/'\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_mixed_mixed(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        # for i in range(window_size):\n",
    "        #     example_output[f\"{unit}_{1 + i + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\", \"temprature\": \"...\"}\n",
    "        example_output[f\"{unit}_{1 + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\", \"temprature\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary and the temprature for {window}, please predict the next 1 day's temprature and weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary_temp', 'fut_summary_temp']].rename(columns={'summary_temp': 'input', 'fut_summary_temp': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_mixed_mixed(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "\n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-mixed-mixed-dc\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-mixed-mixed-cal\", \"mixed-mixed-cal\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8412a07a6d495ab34460878250dd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c76bcdce1f74bfdb85db5216af3a509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebb4f9e6d834778a3b799c199e38d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa8789443d2408f94cb1e64b18453d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09e21aef024485583d123ea560e1b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5e0351a5045d8ab92d63a141a7ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bbd8c00c7e40f49eca0a7a842b4331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865ca390266427ab1b94be24a15f775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51075dd405a8433bb045d5cfc42f03a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051d32584674498294aac08b9a5a6ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d396bb605803473ca489ff31c10cbb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0052e20f2cf043b39851e263cebcd8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ee2cc669b43ff8f45e9c7bdf35895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da2a459125c4f5ab9ace129a5e60a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3961fe835e401794d6775df206801b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db04fe051b04cdd8ceda85ef07c18c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28789c73950f481d818ba0be0ca1cbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f53b3eaa17c4f6182187ba8fda9cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0ff760f2d241d3b2b9ac8acd9b714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9e3eef3bc4eeab8069eefe5bbeb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42557fb50a5a453abd47f217b7cb0057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/DC/temp-only/'\n",
    "\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_text_text(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        # for i in range(window_size):\n",
    "        #     example_output[f\"{unit}_{1 + i + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\"}\n",
    "        example_output[f\"{unit}_{1 + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary for {window}, please predict the next 1 day's weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_text_text(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "            \n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-text-text-dc\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-text-text\", \"text-text\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b91ce768dc84421a69c284a3aeb64d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6af13bf6d14c90be13118f132add40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212240e5acc64371b0809b63bde8d854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbf942aada6457a972c3db3cdd47ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b181e1a0bc946aa950e9953b43e7f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18be0746b1194c82b5fad422a7520ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed8f0794c9f41b487edf598bc6ab5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Howard881010/climate-dc/commit/f7f2ef826beb3bccbbb7b44069fd46d27915d3a4', commit_message='Upload dataset', commit_description='', oid='f7f2ef826beb3bccbbb7b44069fd46d27915d3a4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/DC/temp-only/'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    for size in range(1, window_size+1):\n",
    "        df1 = create_text_text(filename, size, \"day\")\n",
    "        df2 = create_mixed_mixed(filename, size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df1)\n",
    "            dataframe_test.append(df2)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df1)\n",
    "            dataframe_train.append(df2)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df1)\n",
    "            dataframe_val.append(df2)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate-dc\", token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the maximum token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tokens: 1386\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Replace 'mistral-7b-instruct' with the actual model name if available\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hf_dataset = f\"Howard881010/climate-cal-multi\"\n",
    "dataset = load_dataset(hf_dataset, split=\"test\")\n",
    "data = pd.DataFrame(dataset)\n",
    "tokens = []\n",
    "for idx, row in data.iterrows():\n",
    "    token = tokenizer.encode(row['input'])\n",
    "    num_tokens = len(token)\n",
    "    tokens.append(num_tokens)\n",
    "# Retrieve the maximum number of tokens\n",
    "\n",
    "print(f\"Maximum number of tokens: {max(tokens)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
