{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "from utils import split_data, convert_to_parquet, combine_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/numerical_2021-05-04_2023-12-04.csv\"\n",
    "# Path to the directory containing the CSV files\n",
    "climate_data = pd.read_csv(file_path)\n",
    "df = climate_data[['name', 'datetime', 'temp', 'longitude']]\n",
    "\n",
    "result = []\n",
    "# sort the state by longitude from left to right\n",
    "latitude_dict = df.set_index('name')['longitude'].to_dict()\n",
    "sorted_states = sorted(latitude_dict.keys(), key = lambda x: latitude_dict[x])\n",
    "\n",
    "# Group by 'datetime' and create the dictionary for 'temp' for each group\n",
    "for date, group in df.groupby('datetime'):\n",
    "    temp_dict = group.set_index('name')['temp'].to_dict()\n",
    "    \n",
    "    sorted_temp = {state: temp_dict[state] for state in sorted_states}\n",
    "    result.append({'datetime': date, 'temp': sorted_temp})\n",
    "    # only for california\n",
    "    # cal_temp = {\"California\": temp_dict['California']}\n",
    "    # result.append({'datetime': date, 'temp': cal_temp})\n",
    "    \n",
    "    \n",
    "\n",
    "# Create the final DataFrame\n",
    "new_df = pd.DataFrame(result)\n",
    "new_df.to_csv('/home/ubuntu/multimodal/Dataset/Climate-raw/procoess_num_cal.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_of_week(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_of_week = date_obj.strftime('%A')\n",
    "    return f\"{date_str} ({day_of_week})\"\n",
    "\n",
    "def collapse_json(data):\n",
    "    collapsed_string = \"\"\n",
    "    data = ast.literal_eval(data)\n",
    "    for key, values in data.items():\n",
    "        collapsed_string += f\"{key}: \" + \" \".join(values)\n",
    "    collapsed_string = collapsed_string.replace(\"_\", \" \")\n",
    "    return collapsed_string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    {\"Time\": \"2021-05-04 (Tuesday)\", \"temperature\"...\n",
      "4    {\"Time\": \"2021-05-05 (Wednesday)\", \"temperatur...\n",
      "5    {\"Time\": \"2021-05-06 (Thursday)\", \"temperature...\n",
      "6    {\"Time\": \"2021-05-07 (Friday)\", \"temperature\":...\n",
      "7    {\"Time\": \"2021-05-08 (Saturday)\", \"temperature...\n",
      "Name: temp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/summarized_temperature_v1.csv\"\n",
    "num_file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/procoess_num.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)   # from 05-01 - 12-01 \n",
    "df_num = pd.read_csv(num_file_path)  # from 05-04 - 12-04\n",
    "\n",
    "df = df.iloc[3:]\n",
    "df_num = df_num.iloc[:-3]\n",
    "\n",
    "df['temp'] = df_num['temp'].apply(ast.literal_eval)\n",
    "\n",
    "df['fut_temp'] = df['temp'].shift(-1)\n",
    "df['summary'] = df['summary'].apply(collapse_json)\n",
    "df['fut_summary'] = df['summary'].shift(-1)\n",
    "df['Time'] = df['Time'].apply(add_day_of_week)\n",
    "df['fut_Time'] = df['Time'].shift(-1)\n",
    "df['summary_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary'], \"temperature\": x['temp']}), axis=1)\n",
    "\n",
    "df['summary_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary'], \"temperature\": x['temp']}), axis=1)\n",
    "df['fut_summary_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['fut_Time'], \"summary\": x['fut_summary'], \"temperature\": x['fut_temp']}), axis=1)\n",
    "df['temp'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"temperature\": x['temp']}), axis=1)\n",
    "df['fut_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['fut_Time'], \"temperature\": x['fut_temp']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary']}), axis=1)\n",
    "df['fut_summary'] = df.apply(lambda x: json.dumps({\"Time\": x['fut_Time'], \"summary\": x['fut_summary']}), axis=1)\n",
    "\n",
    "print(df['temp'].head(5))\n",
    "\n",
    "\n",
    "df_temp = df[['fut_summary', 'fut_temp', 'temp', 'summary', 'fut_summary_temp', 'summary_temp']]\n",
    "df_temp.to_csv('/home/ubuntu/multimodal/Dataset/Climate/temp.csv', index=False)\n",
    "\n",
    "file_path = '/home/ubuntu/multimodal/Dataset/Climate/temp.csv'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "split_data(file_path)  # Adjust the directory path as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/ubuntu/multimodal/Dataset/Climate/temp_cal.csv'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "split_data(file_path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae59fc404d944e79038003880b7f5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd5323a296d4950b5b125b1ef511f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb80033ab47d4c19a817c03b43256b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf85ece850a949989cfdded560f6b096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc88e4f308ea4446ab521150a47dd142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7c43dfcd6e45d7a9c33a80362953d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c18df672c2541febf95953b8e64f181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d266e3c3e64e15aabee2f1584b3750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e3e8929af44f46b7c7f3a7495f7cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac509d4e9fc041db91531b90d508e577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_mixed_mixed(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        example_output[f\"{unit}_{1 + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\", \"temprature\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary and the temprature of california for {window}, please predict the next 1 day's temprature of 50 states and weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary_temp', 'fut_summary_temp']].rename(columns={'summary_temp': 'input', 'fut_summary_temp': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_mixed_mixed(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "\n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-mixed-mixed-cal\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-mixed-mixed-cal\", \"mixed-mixed-cal\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d3c9501d9d4842abc037ab2990f4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd55e3e546e749a3ae7c365c34b40c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ac119df84b47c99e9b0f6373357f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4dcc65d3b048708f044a3399afe675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d898eb6bf0b94f4da5b99b23e80b0efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a4dd6d432049c79ebe3eabbae0fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a140f57b334fb2a275e7498473e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773576629eb54b02b2828dcc545c43ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427f5b00c70482f8f0489c3889026ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297369468b544b93aaa12da0d22e106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_text_text(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        example_output[f\"{unit}_{1+window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary for {window}, please predict the next 1 day's weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_text_text(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "            \n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-text-text\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-text-text\", \"text-text\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87857e5f70574561987f3c122f6c368d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9252db7124df414f83059b16c574b3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588f551a63c24ac38043b05a9c7a6419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d1875e02eb486181c0cc2e34f2e1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af43abc7e9204814b522b502e66efd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634ac2f5bc23427e8de27df3a5cec49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7bd94bfa2b485fb024f3ed23a5501c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18f303361134a63bd723915d6f207bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da27c4bee45548768109fbb896cd5d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84486982e96464c9bedb63de29babb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Howard881010/climate-cal/commit/72a27e86ebaef59cb116afec08aedc07ec3a8e5a', commit_message='Upload dataset', commit_description='', oid='72a27e86ebaef59cb116afec08aedc07ec3a8e5a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    for size in range(1, window_size+1):\n",
    "        df1 = create_text_text(filename, size, \"day\")\n",
    "        df2 = create_mixed_mixed(filename, size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df1)\n",
    "            dataframe_test.append(df2)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df1)\n",
    "            dataframe_train.append(df2)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df1)\n",
    "            dataframe_val.append(df2)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate-cal\", token=token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
