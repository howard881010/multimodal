{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                               text  \\\n",
      "0  2014-04-04  ...Metadata...\\nEXTENDED FORECAST DISCUSSION\\n...   \n",
      "1  2014-04-05  ...Metadata...\\nEXTENDED FORECAST DISCUSSION\\n...   \n",
      "2  2014-04-06  ...Metadata...\\nEXTENDED FORECAST DISCUSSION\\n...   \n",
      "3  2014-04-07  ...Metadata...\\nEXTENDED FORECAST DISCUSSION\\n...   \n",
      "4  2014-04-08  ...Metadata...\\nEXTENDED FORECAST DISCUSSION\\n...   \n",
      "\n",
      "                                          temp  \\\n",
      "0  {\"date\": \"2014-04-04\", \"temperature\": 54.9}   \n",
      "1  {\"date\": \"2014-04-05\", \"temperature\": 53.1}   \n",
      "2  {\"date\": \"2014-04-06\", \"temperature\": 49.5}   \n",
      "3  {\"date\": \"2014-04-07\", \"temperature\": 48.4}   \n",
      "4  {\"date\": \"2014-04-08\", \"temperature\": 58.3}   \n",
      "\n",
      "                                      fut_temp  \\\n",
      "0  {\"date\": \"2014-04-05\", \"temperature\": 53.1}   \n",
      "1  {\"date\": \"2014-04-06\", \"temperature\": 49.5}   \n",
      "2  {\"date\": \"2014-04-07\", \"temperature\": 48.4}   \n",
      "3  {\"date\": \"2014-04-08\", \"temperature\": 58.3}   \n",
      "4  {\"date\": \"2014-04-09\", \"temperature\": 56.8}   \n",
      "\n",
      "                                         fut_summary    fut_date  \\\n",
      "0  {\"date\": \"2014-04-05\", \"summary\": \"...Metadata...  2014-04-05   \n",
      "1  {\"date\": \"2014-04-06\", \"summary\": \"...Metadata...  2014-04-06   \n",
      "2  {\"date\": \"2014-04-07\", \"summary\": \"...Metadata...  2014-04-07   \n",
      "3  {\"date\": \"2014-04-08\", \"summary\": \"...Metadata...  2014-04-08   \n",
      "4  {\"date\": \"2014-04-09\", \"summary\": \"...Metadata...  2014-04-09   \n",
      "\n",
      "                                        summary_temp  \\\n",
      "0  {\"date\": \"2014-04-04\", \"summary\": \"...Metadata...   \n",
      "1  {\"date\": \"2014-04-05\", \"summary\": \"...Metadata...   \n",
      "2  {\"date\": \"2014-04-06\", \"summary\": \"...Metadata...   \n",
      "3  {\"date\": \"2014-04-07\", \"summary\": \"...Metadata...   \n",
      "4  {\"date\": \"2014-04-08\", \"summary\": \"...Metadata...   \n",
      "\n",
      "                                    fut_summary_temp  \\\n",
      "0  {\"date\": \"2014-04-05\", \"summary\": \"...Metadata...   \n",
      "1  {\"date\": \"2014-04-06\", \"summary\": \"...Metadata...   \n",
      "2  {\"date\": \"2014-04-07\", \"summary\": \"...Metadata...   \n",
      "3  {\"date\": \"2014-04-08\", \"summary\": \"...Metadata...   \n",
      "4  {\"date\": \"2014-04-09\", \"summary\": \"...Metadata...   \n",
      "\n",
      "                                             summary  \n",
      "0  {\"date\": \"2014-04-04\", \"summary\": \"...Metadata...  \n",
      "1  {\"date\": \"2014-04-05\", \"summary\": \"...Metadata...  \n",
      "2  {\"date\": \"2014-04-06\", \"summary\": \"...Metadata...  \n",
      "3  {\"date\": \"2014-04-07\", \"summary\": \"...Metadata...  \n",
      "4  {\"date\": \"2014-04-08\", \"summary\": \"...Metadata...  \n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/temp.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['fut_temp'] = df['temp'].shift(-1)\n",
    "df['fut_summary'] = df['text'].shift(-1)\n",
    "df['fut_date'] = df['date'].shift(-1)\n",
    "\n",
    "df['summary_temp'] = df.apply(lambda x: json.dumps({\"date\": x['date'], \"summary\": x['text'], \"temperature\": x['temp']}), axis=1)\n",
    "df['fut_summary_temp'] = df.apply(lambda x: json.dumps({\"date\": x['fut_date'], \"summary\": x['fut_summary'], \"temperature\": x['fut_temp']}), axis=1)\n",
    "df['temp'] = df.apply(lambda x: json.dumps({\"date\": x['date'], \"temperature\": x['temp']}), axis=1)\n",
    "df['fut_temp'] = df.apply(lambda x: json.dumps({\"date\": x['fut_date'], \"temperature\": x['fut_temp']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"date\": x['date'], \"summary\": x['text']}), axis=1)\n",
    "df['fut_summary'] = df.apply(lambda x: json.dumps({\"date\": x['fut_date'], \"summary\": x['fut_summary']}), axis=1)\n",
    "\n",
    "df.to_csv('/home/ubuntu/multimodal/Dataset/Climate/temp.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path, train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the validation and test sizes\n",
    "    val_size = validation_ratio / (test_ratio + validation_ratio)\n",
    "    \n",
    "    # Split the data into train and temporary datasets\n",
    "    train_data, temp_data = train_test_split(data, test_size=(1 - train_ratio), random_state=42, shuffle=False)\n",
    "    \n",
    "    # Split the temporary dataset into validation and test datasets\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=val_size, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Save the datasets\n",
    "    \n",
    "    dir = file_path.split('/')[:-1]\n",
    "    dir = '/'.join(dir)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    train_data.to_csv(f'{dir}/train_{file_name}', index=False)\n",
    "    validation_data.to_csv(f'{dir}/val_{file_name}', index=False)\n",
    "    test_data.to_csv(f'{dir}/test_{file_name}', index=False)\n",
    "\n",
    "# List of your CSV files\n",
    "dir = '/home/ubuntu/multimodal/Dataset/Climate'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "for filename in os.listdir(dir):\n",
    "    if not filename.startswith(\"train\") and not filename.startswith(\"test\") and not filename.startswith(\"val\"):\n",
    "        path = os.path.join(dir, filename)\n",
    "        split_data(path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(dataframe_test, dataframe_train, dataframe_val):\n",
    "    train = pd.concat(dataframe_train)\n",
    "    test = pd.concat(dataframe_test)\n",
    "    val = pd.concat(dataframe_val)\n",
    "\n",
    "    train_path = '../parquet_dir/train_finance.parquet'\n",
    "    test_path = '../parquet_dir/test_finance.parquet'\n",
    "    val_path = '../parquet_dir/val_finance.parquet'\n",
    "\n",
    "    train.to_parquet(train_path, engine='pyarrow')\n",
    "    test.to_parquet(test_path, engine='pyarrow')\n",
    "    val.to_parquet(val_path, engine='pyarrow')\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset('parquet', data_files=train_path, split='train')\n",
    "    test_dataset = load_dataset('parquet', data_files=test_path, split='train')\n",
    "    val_dataset = load_dataset('parquet', data_files=val_path, split = 'train')\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_huggingface(dataset_path, case, units, dataset_name):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "    if not os.path.exists(f\"../Data/{dataset_name}/{units}/{case}\"):\n",
    "        os.makedirs(f\"../Data/{dataset_name}/{units}/{case}\")\n",
    "\n",
    "# Access the train split\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        train_dataset = dataset[split]\n",
    "\n",
    "        # Convert the dataset to a Pandas DataFrame\n",
    "        df = train_dataset.to_pandas()\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f\"../Data/{dataset_name}/{units}/{case}/{split}_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_window(df, window_size, unit):\n",
    "    json_data = []\n",
    "    end_index = len(df) - 2 * window_size + 1\n",
    "\n",
    "\n",
    "    for i in range(end_index):\n",
    "        combined_input = {}\n",
    "        combine_output = {}\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            input_key = f\"{unit}_{j+1}\"\n",
    "            output_key = f\"{unit}_{j+window_size+1}\"\n",
    "            combined_input[input_key] = json.loads(df.iloc[i + j]['input'])\n",
    "            combine_output[output_key] = json.loads(df.iloc[i + j + window_size - 1]['output'])\n",
    "        combine_json = {\n",
    "            \"input\": json.dumps(combined_input),\n",
    "            \"output\": json.dumps(combine_output),\n",
    "            \"instruction\": df.iloc[i]['instruction']\n",
    "        }\n",
    "        json_data.append(combine_json)\n",
    "    \n",
    "    json_df = pd.DataFrame(json_data)\n",
    "    return json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa5a02934014583852356a9bf4103ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289918cac3044af3ae6a9d312cfb372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc01736d4b45c19d362623709ceb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c204f6e8a4a04d54945ff3b269b73ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94da9623d174497892347ae712f5ed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd387cf4a146c78d17ba991030182b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3c5e0c052647af995caad5645b0da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2c19b88d554560bc26194cfb4028e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dedf10b4a97457caf5a5b8d131a6037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5f8a003c1c4f16a78cdfa8b30acfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96b3d895cc844a6a40f7759fcf89891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5d0c14635544cbbcd694d2e9e86271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bab7ee7702c4720bac44a5d6843f766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97edb6f835b432fb0a389d050eed8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c21539a79440e7926ea5b783a77fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f774acaef064d3a8b63d6257abe38a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241aeffcd97b45fda8e2931c11654777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "def create_mixed_mixed(filename, window_size, unit):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"{unit}_{i+1+window_size}\"] = {\"date\": \"<date>\", \"summary\": \"<summary>\", \"temprature\": \"<temprature>\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the summary and the temprature for {window}, please predict the next {window}'s temprature and summary in json format. And ouput only the json data, the response should only contain {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary_temp', 'fut_summary_temp']].rename(columns={'summary_temp': 'input', 'fut_summary_temp': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    df = create_mixed_mixed(filename, window_size, \"day\")\n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-mixed-mixed\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(f\"Howard881010/climate-{window_size}_day-mixed-mixed\", \"mixed-mixed\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa837a0998794d299b6293fbf4150d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e928d522c04d399c5957fff04b3fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc73839069c443b8ee762436c76553e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6c4c3838c6449a94c09c94fbc66b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bddc516cf04061a846cb4477621c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d671f9344c224e28a34857cc5202784e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74330510ead646e5a6df1c4131e8cde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dfe0397d964234b703334c86a9f587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92428b45fe1046adb9ada8d66e1075d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f89800e432c4bc1912e613f12be13c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1d296914a0488eb724c4080ad7555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85381af74caa478096a7aa7e839b503e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06750a96d65b4d54b8ed7815a38ef7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a0b746c9eb4ffdaa89186ab8175a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8774c5d2ce9c4d01ac9ebf3497bcfbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790b125c102c4df480ea6e25e0708682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "def create_text_text(filename, window_size, unit):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"{unit}_{i+1+window_size}\"] = {\"date\": \"<date>\", \"summary\": \"<summary>\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the summary for {window}, please predict the next {window}'s summary in json format. And ouput only the json data, the response should only contain {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window(df, window_size, \"day\")\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    df = create_text_text(filename, window_size, \"day\")\n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-text-text\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(f\"Howard881010/climate-{window_size}_day-text-text\", \"text-text\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b916ba98494a34be69eed39f8cc6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a8bc8ca2ef43199cbfdf85421844e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf809a5a598e4c53bcf4354aa76a9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2174b24989c34095a35052cf1fce235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee5e92e56c49bcb1ae21eadab5118a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2092b8a963864111bafee71e6a0530f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35231cbbfde3475a98f819fcb5c0d2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fa5bc1cf824fe89aadf23cfa9408ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76aae31b43e4ef4888796841bae93ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Howard881010/climate/commit/162186f448bc4a5a734ef15afb70d30489c9802d', commit_message='Upload dataset', commit_description='', oid='162186f448bc4a5a734ef15afb70d30489c9802d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    for size in range(1, window_size+1):\n",
    "        df1 = create_text_text(filename, size, \"day\")\n",
    "        df2 = create_mixed_mixed(filename, size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df1)\n",
    "            dataframe_test.append(df2)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df1)\n",
    "            dataframe_train.append(df2)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df1)\n",
    "            dataframe_val.append(df2)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate\", token=token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
