{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "from utils import split_data, convert_to_parquet, combine_window, combine_window_multiple_output\n",
    "import ast\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/numerical_2021-05-04_2023-12-04.csv\"\n",
    "# Path to the directory containing the CSV files\n",
    "climate_data = pd.read_csv(file_path)\n",
    "df = climate_data[['name', 'datetime', 'temp', 'longitude']]\n",
    "\n",
    "result = []\n",
    "# sort the state by longitude from left to right\n",
    "latitude_dict = df.set_index('name')['longitude'].to_dict()\n",
    "sorted_states = sorted(latitude_dict.keys(), key = lambda x: latitude_dict[x])\n",
    "\n",
    "# Group by 'datetime' and create the dictionary for 'temp' for each group\n",
    "for date, group in df.groupby('datetime'):\n",
    "    temp_dict = group.set_index('name')['temp'].to_dict()\n",
    "    \n",
    "    sorted_temp = {state: temp_dict[state] for state in sorted_states}\n",
    "    result.append({'datetime': date, 'temp': sorted_temp})\n",
    "    # only for california\n",
    "    # cal_temp = {\"California\": temp_dict['California']}\n",
    "    # result.append({'datetime': date, 'temp': cal_temp})\n",
    "    \n",
    "    \n",
    "\n",
    "# Create the final DataFrame\n",
    "new_df = pd.DataFrame(result)\n",
    "new_df.to_csv('/home/ubuntu/multimodal/Dataset/Climate-raw/procoess_num.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_of_week(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    day_of_week = date_obj.strftime('%A')\n",
    "    return f\"{date_str} ({day_of_week})\"\n",
    "\n",
    "def collapse_json(data):\n",
    "    collapsed_string = \"\"\n",
    "    data = ast.literal_eval(data)\n",
    "    for key, values in data.items():\n",
    "        collapsed_string += f\"{key}: \" + \" \".join(values) + \". \"\n",
    "    collapsed_string = collapsed_string.replace(\"_\", \" \")\n",
    "    return collapsed_string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    2021-05-04\n",
      "4    2021-05-05\n",
      "5    2021-05-06\n",
      "6    2021-05-07\n",
      "7    2021-05-08\n",
      "Name: Time, dtype: object\n",
      "0    2021-05-04\n",
      "1    2021-05-05\n",
      "2    2021-05-06\n",
      "3    2021-05-07\n",
      "4    2021-05-08\n",
      "Name: datetime, dtype: object\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/summarized_temperature_v1.csv\"\n",
    "num_file_path = \"/home/ubuntu/multimodal/Dataset/Climate-raw/temp_cal.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)   # from 05-01 - 12-01 \n",
    "df_num = pd.read_csv(num_file_path)  # from 05-04 - 12-04\n",
    "\n",
    "# so the overall dataset is from 05-04 - 12-01\n",
    "\n",
    "df = df.iloc[3:]\n",
    "print(df['Time'].head(5))\n",
    "print(df_num['datetime'].head(5))\n",
    "\n",
    "df['temp'] = df_num['temp'].apply(ast.literal_eval).shift(3)\n",
    "df['summary'] = df['summary'].apply(collapse_json)\n",
    "df['Time'] = df['Time'].apply(add_day_of_week)\n",
    "df['summary_temp'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary'], \"temperature\": x['temp']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"Time\": x['Time'], \"summary\": x['summary']}), axis=1)\n",
    "\n",
    "df['fut_summary'] = df['summary'].shift(-1)\n",
    "df['fut_summary_temp'] = df['summary_temp'].shift(-1)\n",
    "\n",
    "\n",
    "df_temp = df[['summary', 'fut_summary', 'summary_temp', 'fut_summary_temp']]\n",
    "df_temp.to_csv('/home/ubuntu/multimodal/Dataset/Climate/cal/temp-only/temp_cal.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/ubuntu/multimodal/Dataset/Climate/cal/temp-only/temp_cal.csv'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "split_data(file_path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed66a1d1b1446c1a893cdb436528a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1a90cb3f524f69b60d34711b02e8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa31f57c6bfc4e28abe5da57eba02d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d367be66d4748b9a879ac41fbeb3cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c990568ef64b228c70d0fe1c522cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ca3f058fe64116962e0d3957cfcad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8095185e468a402c960cd8e73740a6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2f44c89dfa42318b4f8a2e26301e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636cca5cc95c491f8a26f6c01a5ab27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4c95e402364ce9b9ef58e7f1c62107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff7fab2529241218239ebe5a02a84f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bea4833b25140ca8e2ceaff06af6ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54fda74d22d4d229ac095565c2803af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fde9af20d4e431f96632b3002a04add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dadd1793d6e4a358b1c31dcd6ae8156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2110bd07be884f28badff60a6ae86796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89fe381d2af40078fbcad2baac675cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af3c4f2bf0d49e88cee52ae8b226fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536346e11a0c414ebc8cc9339bac1f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d09b2aa64844a47864a1454df2d47e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/cal/temp-only/'\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_mixed_mixed(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"{unit}_{1 + i + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\", \"temprature\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary and the temprature for {window}, please predict the next {window} day's temprature and weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary_temp', 'fut_summary_temp']].rename(columns={'summary_temp': 'input', 'fut_summary_temp': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window_multiple_output(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_mixed_mixed(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "\n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-mixed-mixed-cal-multi\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-mixed-mixed-cal\", \"mixed-mixed-cal\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d9390014494419bbdedfa25d0f41db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cc36c61e8647e195c9653c30baa034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59a14ff77104a128cd50e7b322bd2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bc48660f374357b7b3d31e7d8167cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a6a54e226b4dd3a4f745c9ad2927d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1a10ff22c74ab49740278b85e3babc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa175d7201d14dbd8d5183a184f2e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751696e7575b47b59b8da6560d28ec3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0604bef0d843c2ba67710371ecf278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535eeef05cb14fd5b7c4b4b45f2586f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48643e08671b49838bebb439e9d86a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a8492691f440b8804fb28f72f783ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4653837ddec40568cc8b9658ce368de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97448db13dbb4aeb8a9ae083b4f482b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7c51f56e16421f9d435bdbc41be23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8223af67814e91abba843ababe5f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1713d73dbbb4956906cfd89a8b40333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19196a9aee9b471fb2e8212eeaffd750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b9e5563b114ee1bc82ffc06b220689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b665492731a42bbab5caca4fbf60908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00391a02dba4ce8bf0d98f270e402b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/cal/temp-only/'\n",
    "\n",
    "window_sizes = [1, 2, 3]\n",
    "\n",
    "def create_text_text(filename, window_size, unit):\n",
    "    if (filename.startswith('train') or filename.startswith('test') or filename.startswith('val')):\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = f\"{window_size} {unit}\"\n",
    "        else:\n",
    "            window = f\"{window_size} {unit}s\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"{unit}_{1 + i + window_size}\"] = {\"Time\": \"...\", \"summary\": \"...\"}\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the weather summary for {window}, please predict the next {window_size} day's weather summary within a JSON. The example output is {example_output}\"\n",
    "        \n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        df['pred_output'] = \"Not available\"\n",
    "        # skip the first and last historical_size days\n",
    "        return combine_window_multiple_output(df, window_size, \"day\")\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    dataframe_train = []\n",
    "    dataframe_test = []\n",
    "    dataframe_val = []\n",
    "    for filename in os.listdir(dir):\n",
    "        df = create_text_text(filename, window_size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df)\n",
    "            \n",
    "    dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    # Push the dataset to the Hugging Face Hub\n",
    "    dataset_dict.push_to_hub(f\"Howard881010/climate-{window_size}_day-text-text-cal-multi\", token=token)\n",
    "\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    # load_from_huggingface(f\"Howard881010/climate-{window_size}_day-text-text\", \"text-text\", f\"{window_size}_day\", \"Climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e03ce71747448ab0aa070006f92c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d4c60c526a48c691a2b6821a2459fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2526937ab56c47128a3fbb22f45719e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20776f261c7a4c3cb8ad038574897612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76318d7ebc974fc0a8b61bbf08b6239b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0508d8dfb164d43bfbfc428597efcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f12c1e147b42358f8ce7b52f7cb159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Howard881010/climate-cal-multi/commit/a060d415b4f7a2dbc5fe139c1e16bcad71608860', commit_message='Upload dataset', commit_description='', oid='a060d415b4f7a2dbc5fe139c1e16bcad71608860', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = '../Dataset/Climate/cal/temp-only/'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    for size in range(1, window_size+1):\n",
    "        df1 = create_text_text(filename, size, \"day\")\n",
    "        df2 = create_mixed_mixed(filename, size, \"day\")\n",
    "        if filename.startswith('test'):\n",
    "            dataframe_test.append(df1)\n",
    "            dataframe_test.append(df2)\n",
    "        elif filename.startswith('train'):\n",
    "            dataframe_train.append(df1)\n",
    "            dataframe_train.append(df2)\n",
    "        elif filename.startswith('val'):\n",
    "            dataframe_val.append(df1)\n",
    "            dataframe_val.append(df2)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/climate-cal-multi\", token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the maximum token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tokens: 1386\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Replace 'mistral-7b-instruct' with the actual model name if available\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hf_dataset = f\"Howard881010/climate-cal-multi\"\n",
    "dataset = load_dataset(hf_dataset, split=\"test\")\n",
    "data = pd.DataFrame(dataset)\n",
    "tokens = []\n",
    "for idx, row in data.iterrows():\n",
    "    token = tokenizer.encode(row['input'])\n",
    "    num_tokens = len(token)\n",
    "    tokens.append(num_tokens)\n",
    "# Retrieve the maximum number of tokens\n",
    "\n",
    "print(f\"Maximum number of tokens: {max(tokens)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
