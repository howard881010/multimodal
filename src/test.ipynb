{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic score: 0.18334531784057617\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I love you\"\n",
    "sentence2 = \"You score a terrible 1!\"\n",
    "\n",
    "# Encode the sentences\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "\n",
    "print(f\"Semantic score: {cosine_score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(key)\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. And can give me the semantic score between two summary from 1 to 10, you should only give me the scores\"},\n",
    "    {\"role\": \"user\", \"content\": \"summary1: I love you, summary2: I really love you!\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import clean_num\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPTScore(df, num_key_name):\n",
    "    key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=key)\n",
    "\n",
    "    df.replace(\"Wrong output format\", np.nan, inplace=True)\n",
    "    \n",
    "    df_clean = df.dropna()\n",
    "    df_text_part = clean_num(df_clean, num_key_name)\n",
    "\n",
    "    gpt_scores = []    \n",
    "    for idx, row in df_text_part.iterrows():\n",
    "        question = f\"summary1: {row['output']} summary2: {row['pred_output']}\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. And can give me the semantic score between two summary from 1 to 10, you should only give me the scores\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        )\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        # gpt_scores.append(x)\n",
    "        if idx == 10:\n",
    "            break\n",
    "    \n",
    "    return gpt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ea8d9bd0a34cf29aca40917d5f8706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82ca23b4fce4533b06c548fed777d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/467k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8f9580af0c417e99247627b99398a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/106k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48ab57ced0f42f08c2be2bbae429401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/106k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bb418a9e2c45668b5c5b63f1cf456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a60c2ce10dc404cadfe49da3c688bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c731669601ff4a87af0dbf2f8a23a8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'ChatCompletion' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m data_all \u001b[38;5;241m=\u001b[39m load_dataset(hf_dataset)\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m gpt_scores \u001b[38;5;241m=\u001b[39m \u001b[43mgetGPTScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgas_price\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mgetGPTScore\u001b[0;34m(df, num_key_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m summary2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m gpt_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "hf_dataset = \"Howard881010/gas-1_week-mixed-mixed-west\"\n",
    "data_all = load_dataset(hf_dataset)\n",
    "data = pd.DataFrame(data_all['validation'])\n",
    "\n",
    "gpt_scores = getGPTScore(data, \"gas_price\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
