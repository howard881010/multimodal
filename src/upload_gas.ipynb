{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Gas-raw/West_Coast_fact.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['fut_price'] = df['Weekly West Coast All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)'].shift(-1)\n",
    "df['fut_summary'] = df['text']\n",
    "df['summary'] = df['text'].shift(1)\n",
    "\n",
    "df['time_period'] = df.apply(lambda x: x['start_date'] + \" - \" + x['end_date'], axis=1)\n",
    "df['fut_time_period'] = df['time_period'].shift(-1)\n",
    "# df = df.rename(columns={'text': 'summary'})\n",
    "df = df.rename(columns={'Weekly West Coast All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'price'})\n",
    "df['summary_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"summary\": x['summary'], \"gas_price\": x['price']}), axis=1)\n",
    "df['fut_summary_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"summary\": x['fut_summary'], \"gas_price\": x['fut_price']}), axis=1)\n",
    "df['price'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"gas_price\": x['price']}), axis=1)\n",
    "df['fut_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"gas_price\": x['fut_price']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"summary\": x['summary']}), axis=1)\n",
    "df['fut_summary'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"summary\": x['fut_summary']}), axis=1)\n",
    "\n",
    "# print(df.head(5))\n",
    "df.to_csv('/home/ubuntu/multimodal/Dataset/Gas/West_Coast_fact.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path, train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the validation and test sizes\n",
    "    val_size = validation_ratio / (test_ratio + validation_ratio)\n",
    "    \n",
    "    # Split the data into train and temporary datasets\n",
    "    train_data, temp_data = train_test_split(data, test_size=(1 - train_ratio), random_state=42, shuffle=False)\n",
    "    \n",
    "    # Split the temporary dataset into validation and test datasets\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=val_size, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Save the datasets\n",
    "    \n",
    "    dir = file_path.split('/')[:-1]\n",
    "    dir = '/'.join(dir)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    train_data.to_csv(f'{dir}/train_{file_name}', index=False)\n",
    "    validation_data.to_csv(f'{dir}/val_{file_name}', index=False)\n",
    "    test_data.to_csv(f'{dir}/test_{file_name}', index=False)\n",
    "\n",
    "# List of your CSV files\n",
    "dir = '/home/ubuntu/multimodal/Dataset/Gas/fact_only'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "for filename in os.listdir(dir):\n",
    "    if not filename.startswith(\"train\") and not filename.startswith(\"test\") and not filename.startswith(\"val\"):\n",
    "        path = os.path.join(dir, filename)\n",
    "        split_data(path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(dataframe_test, dataframe_train, dataframe_val):\n",
    "    train = pd.concat(dataframe_train)\n",
    "    test = pd.concat(dataframe_test)\n",
    "    val = pd.concat(dataframe_val)\n",
    "\n",
    "    train_path = '../parquet_dir/train_finance.parquet'\n",
    "    test_path = '../parquet_dir/test_finance.parquet'\n",
    "    val_path = '../parquet_dir/val_finance.parquet'\n",
    "\n",
    "    train.to_parquet(train_path, engine='pyarrow')\n",
    "    test.to_parquet(test_path, engine='pyarrow')\n",
    "    val.to_parquet(val_path, engine='pyarrow')\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset('parquet', data_files=train_path, split='train')\n",
    "    test_dataset = load_dataset('parquet', data_files=test_path, split='train')\n",
    "    val_dataset = load_dataset('parquet', data_files=val_path, split = 'train')\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_huggingface(dataset_path, case, units):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "    if not os.path.exists(f\"../Data/Gas/{units}/{case}\"):\n",
    "        os.makedirs(f\"../Data/Gas/{units}/{case}\")\n",
    "\n",
    "# Access the train split\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        train_dataset = dataset[split]\n",
    "\n",
    "        # Convert the dataset to a Pandas DataFrame\n",
    "        df = train_dataset.to_pandas()\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f\"../Data/Gas/{units}/{case}/{split}_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_window(df, window_size, unit):\n",
    "    json_data = []\n",
    "    end_index = len(df) - 2 * window_size + 1\n",
    "\n",
    "\n",
    "    for i in range(end_index):\n",
    "        combined_input = {}\n",
    "        combine_output = {}\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            input_key = f\"{unit}_{j+1}\"\n",
    "            output_key = f\"{unit}_{j+window_size+1}\"\n",
    "            combined_input[input_key] = json.loads(df.iloc[i + j]['input'])\n",
    "            combine_output[output_key] = json.loads(df.iloc[i + j + window_size - 1]['output'])\n",
    "        combine_json = {\n",
    "            \"input\": json.dumps(combined_input),\n",
    "            \"output\": json.dumps(combine_output),\n",
    "            \"instruction\": df.iloc[i]['instruction']\n",
    "        }\n",
    "        json_data.append(combine_json)\n",
    "    \n",
    "    json_df = pd.DataFrame(json_data)\n",
    "    return json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa52d74a2e34c8693de1e029bd3c723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3d3a02175341c48211edd08491e69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199c40f76130434f9b33c5414e670642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec347ebe48c45f497c9218e3d7d84c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72af7a381d246d08ac7e0940a4d7f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bfcaf251674c6296b3e906c7847495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1939bcc1fd48fc8353c57e6ed03eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049c31a054dd497796163d9607a15569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6272e93a1a04033b4003a45a1ab94b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca41189f4334049832f687a4e584578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e303300e2b4d958769d2a14bf784d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1613564b964111982588b9285f904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/290k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31733b5cdd4644c286569490fb9e5f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb72215580bc41909ffc4822124b416b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407e455c24d4b1ea532ab38b2290f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2797692700884fda81815e0722886138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54498ed9679d4a50af19d2db39a77dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Gas/fact_only'\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = \"1 week\"\n",
    "        else:\n",
    "            window = f\"{window_size} weeks\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"week_{i+1+window_size}\"] = {\"time-period\": \"<time-period>\", \"summary\": \"<summary>\", \"gas_price\": \"<gas_price>\"}\n",
    "        # example_output = \"{\\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}, ... \\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}}\"\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the summary and the gas price for {window}, please predict the next {window}'s gas price and summary in json format. And ouput only the json data, the response should only contain {example_output}\"\n",
    "\n",
    "        df = summaries[['summary_price', 'fut_summary_price']].rename(columns={'summary_price': 'input', 'fut_summary_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    df = combine_window(df, window_size, \"week\")\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/gas-{window_size}_week-mixed-mixed-fact\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(f\"Howard881010/gas-{window_size}_week-mixed-mixed-fact\", \"mixed-mixed-fact\", f\"{window_size}_week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf65fe453bd423bb22de9a8a3793bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27de5305a03d4718b75f24c266af4a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1495a1c7f7c14d55a37f55552a57774e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08944188aebf4666a3907f0f002f2534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7053922c8af24a96b09b7f38012b5086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deab1995294416183d119557a1bc620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0c4999a7e347d9aea905dc3c346b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bda3fe724c4322a30a80ac4d60db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9529534aa4d4d4f90b049a464cb5882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542af7597bc84a7783f33ef7f3200e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016ab0d7c3fe43c0873284cce1120ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24173de7030147de858e6a073946dcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/270k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7fc359d9545d296f1d5e640f2a34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8fbe0a33b462baeb597824c0d97b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a8ff0da4024c44acc4605ce7edf736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc453faea87a410da18a4abfd29d86a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fad983917fd46e59175954b63ea547b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = '../Dataset/Gas/fact_only'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = \"1 week\"\n",
    "        else:\n",
    "            window = f\"{window_size} weeks\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"week_{i+1+window_size}\"] = {\"time-period\": \"<time-period>\", \"summary\": \"<summary>\"}\n",
    "        # example_output = \"{\\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}, ... \\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}}\"\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the summary and time period for {window},please predict next {window} summary in json format. And ouput only the json data, the response should only contain {example_output}\"\n",
    "\n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    df = combine_window(df, window_size, \"week\")\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/gas-{window_size}_week-text-text-fact\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(f\"Howard881010/gas-{window_size}_week-text-text-fact\", \"text-text-fact\", f\"{window_size}_week\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
