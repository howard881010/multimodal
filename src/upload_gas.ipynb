{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/multimodal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import DatasetDict, Dataset, Features, Value\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1993-04-05 - 1993-04-11\n",
      "1    1993-04-12 - 1993-04-18\n",
      "2    1993-04-19 - 1993-04-25\n",
      "3    1993-05-10 - 1993-05-16\n",
      "4    1993-05-31 - 1993-06-06\n",
      "Name: time_period, dtype: object\n",
      "0    1993-04-12 - 1993-04-18\n",
      "1    1993-04-19 - 1993-04-25\n",
      "2    1993-05-10 - 1993-05-16\n",
      "3    1993-05-31 - 1993-06-06\n",
      "4    1993-06-07 - 1993-06-13\n",
      "Name: fut_time_period, dtype: object\n",
      "0    {\"time-period\": \"1993-04-05 - 1993-04-11\", \"ga...\n",
      "1    {\"time-period\": \"1993-04-12 - 1993-04-18\", \"ga...\n",
      "2    {\"time-period\": \"1993-04-19 - 1993-04-25\", \"ga...\n",
      "3    {\"time-period\": \"1993-05-10 - 1993-05-16\", \"ga...\n",
      "4    {\"time-period\": \"1993-05-31 - 1993-06-06\", \"ga...\n",
      "Name: price, dtype: object\n",
      "0    {\"time-period\": \"1993-04-12 - 1993-04-18\", \"ga...\n",
      "1    {\"time-period\": \"1993-04-19 - 1993-04-25\", \"ga...\n",
      "2    {\"time-period\": \"1993-05-10 - 1993-05-16\", \"ga...\n",
      "3    {\"time-period\": \"1993-05-31 - 1993-06-06\", \"ga...\n",
      "4    {\"time-period\": \"1993-06-07 - 1993-06-13\", \"ga...\n",
      "Name: fut_price, dtype: object\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/multimodal/Dataset/Gas-raw/West_Coast.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['fut_price'] = df['Weekly West Coast All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)'].shift(-1)\n",
    "df['fut_summary'] = df['text'].shift(-1)\n",
    "df['time_period'] = df.apply(lambda x: x['start_date'] + \" - \" + x['end_date'], axis=1)\n",
    "df['fut_time_period'] = df['time_period'].shift(-1)\n",
    "print(df['time_period'].head(5))\n",
    "print(df['fut_time_period'].head(5))\n",
    "df = df.rename(columns={'text': 'summary'})\n",
    "df = df.rename(columns={'Weekly West Coast All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)': 'price'})\n",
    "df['summary_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"summary\": x['summary'], \"gas_Price\": x['price']}), axis=1)\n",
    "df['fut_summary_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"summary\": x['fut_summary'], \"gas_Price\": x['fut_price']}), axis=1)\n",
    "df['price'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"gas_Price\": x['price']}), axis=1)\n",
    "df['fut_price'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"gas_Price\": x['fut_price']}), axis=1)\n",
    "df['summary'] = df.apply(lambda x: json.dumps({\"time-period\": x['time_period'], \"summary\": x['summary']}), axis=1)\n",
    "df['fut_summary'] = df.apply(lambda x: json.dumps({\"time-period\": x['fut_time_period'], \"summary\": x['fut_summary']}), axis=1)\n",
    "print(df['price'].head(5))\n",
    "print(df['fut_price'].head(5))\n",
    "\n",
    "\n",
    "# print(df.head(5))\n",
    "df.to_csv('/home/ubuntu/multimodal/Dataset/Gas/West_Coast.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path, train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the validation and test sizes\n",
    "    val_size = validation_ratio / (test_ratio + validation_ratio)\n",
    "    \n",
    "    # Split the data into train and temporary datasets\n",
    "    train_data, temp_data = train_test_split(data, test_size=(1 - train_ratio), random_state=42, shuffle=False)\n",
    "    \n",
    "    # Split the temporary dataset into validation and test datasets\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=val_size, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Save the datasets\n",
    "    \n",
    "    dir = file_path.split('/')[:-1]\n",
    "    dir = '/'.join(dir)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    train_data.to_csv(f'{dir}/train_{file_name}', index=False)\n",
    "    validation_data.to_csv(f'{dir}/val_{file_name}', index=False)\n",
    "    test_data.to_csv(f'{dir}/test_{file_name}', index=False)\n",
    "\n",
    "# List of your CSV files\n",
    "dir = '/home/ubuntu/multimodal/Dataset/Gas'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "for filename in os.listdir(dir):\n",
    "    if not filename.startswith(\"train\") and not filename.startswith(\"test\") and not filename.startswith(\"val\"):\n",
    "        path = os.path.join(dir, filename)\n",
    "        split_data(path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(dataframe_test, dataframe_train, dataframe_val):\n",
    "    train = pd.concat(dataframe_train)\n",
    "    test = pd.concat(dataframe_test)\n",
    "    val = pd.concat(dataframe_val)\n",
    "\n",
    "    train_path = '../parquet_dir/train_finance.parquet'\n",
    "    test_path = '../parquet_dir/test_finance.parquet'\n",
    "    val_path = '../parquet_dir/val_finance.parquet'\n",
    "\n",
    "    train.to_parquet(train_path, engine='pyarrow')\n",
    "    test.to_parquet(test_path, engine='pyarrow')\n",
    "    val.to_parquet(val_path, engine='pyarrow')\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset('parquet', data_files=train_path, split='train')\n",
    "    test_dataset = load_dataset('parquet', data_files=test_path, split='train')\n",
    "    val_dataset = load_dataset('parquet', data_files=val_path, split = 'train')\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_huggingface(dataset_path, case, units):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "    if not os.path.exists(f\"../Data/Gas/{units}/{case}\"):\n",
    "        os.makedirs(f\"../Data/Gas/{units}/{case}\")\n",
    "\n",
    "# Access the train split\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        train_dataset = dataset[split]\n",
    "\n",
    "        # Convert the dataset to a Pandas DataFrame\n",
    "        df = train_dataset.to_pandas()\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f\"../Data/Gas/{units}/{case}/{split}_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_window(df, window_size, unit):\n",
    "    json_data = []\n",
    "    end_index = len(df) - 2 * window_size + 1\n",
    "\n",
    "\n",
    "    for i in range(end_index):\n",
    "        combined_input = {}\n",
    "        combine_output = {}\n",
    "        \n",
    "        for j in range(window_size):\n",
    "            input_key = f\"{unit}{j+1}\"\n",
    "            output_key = f\"{unit}{j+window_size+1}\"\n",
    "            combined_input[input_key] = json.loads(df.iloc[i + j]['input'])\n",
    "            combine_output[output_key] = json.loads(df.iloc[i + j + window_size - 1]['output'])\n",
    "        combine_json = {\n",
    "            \"input\": json.dumps(combined_input),\n",
    "            \"output\": json.dumps(combine_output),\n",
    "            \"instruction\": df.iloc[i]['instruction']\n",
    "        }\n",
    "        json_data.append(combine_json)\n",
    "    \n",
    "    json_df = pd.DataFrame(json_data)\n",
    "    return json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case1 number -> number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../Dataset/Finance/v0.2/'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the share price for the current day, please predict the shared price in json format for next day. The output should be like {\"\"share_price\"\":  <value>}\"\n",
    "\n",
    "        df = summaries[['input_price', 'output_price']].rename(columns={'input_price': 'input', 'output_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "        test_all.append(df)\n",
    "    if filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "        train_all.append(df)\n",
    "    if filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        val_all.append(df)\n",
    "\n",
    "# dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "\n",
    "# token = os.getenv(\"HF_TOKEN\")\n",
    "# # Push the dataset to the Hugging Face Hub\n",
    "# dataset_dict.push_to_hub(f\"Howard881010/finance-numerical\", token=token)\n",
    "\n",
    "\n",
    "# # Load the dataset from Hugging Face Hub\n",
    "# load_from_huggingface(\"Howard881010/finance-numerical\", \"numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 number + text => number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../Dataset/Finance/v0.2/'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the financial report and the share price for the current day, please predict the shared price in json format for next day. The output should be like {\"\"share_price\"\":  <value>}\"\n",
    "\n",
    "        df = summaries[['input', 'output_price']].rename(columns={'output_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "        test_all.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "        train_all.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        val_all.append(df)\n",
    "\n",
    "# dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "# token = os.getenv(\"HF_TOKEN\")\n",
    "# # Push the dataset to the Hugging Face Hub\n",
    "# dataset_dict.push_to_hub(f\"Howard881010/finance-mixed-numerical\", token=token)\n",
    "\n",
    "\n",
    "# # Load the dataset from Hugging Face Hub\n",
    "# load_from_huggingface(\"Howard881010/finance-mixed-numerical\", \"mixed-numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 text + number  => text + number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1297 examples [00:00, 18942.14 examples/s]\n",
      "Generating train split: 163 examples [00:00, 2560.27 examples/s]\n",
      "Generating train split: 162 examples [00:00, 2909.51 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 196.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 478.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 511.00ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Downloading readme: 100%|██████████| 561/561 [00:00<00:00, 2.22MB/s]\n",
      "Downloading data: 100%|██████████| 466k/466k [00:00<00:00, 1.54MB/s]\n",
      "Downloading data: 100%|██████████| 73.8k/73.8k [00:00<00:00, 240kB/s]\n",
      "Downloading data: 100%|██████████| 76.7k/76.7k [00:00<00:00, 317kB/s]\n",
      "Generating train split: 100%|██████████| 1297/1297 [00:00<00:00, 9962.90 examples/s]\n",
      "Generating validation split: 100%|██████████| 162/162 [00:00<00:00, 2786.86 examples/s]\n",
      "Generating test split: 100%|██████████| 163/163 [00:00<00:00, 2975.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dir = '../Dataset/Gas'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the summary of the today's gas price and today's gas price, please predict the summary and gas price for next day in json format. The output should be like {\\\"summary\\\": <summary>, \\\"gas_price\\\": <gas_price>}\"\n",
    "\n",
    "        df = summaries[['summary_price', 'fut_summary_price']].rename(columns={'summary_price': 'input', 'fut_summary_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "\n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/gas-mixed-mixed\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(\"Howard881010/gas-mixed-mixed\", \"mixed-mixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 text => text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1027 examples [00:00, 7369.04 examples/s]\n",
      "Generating train split: 121 examples [00:00, 1488.30 examples/s]\n",
      "Generating train split: 120 examples [00:00, 1530.95 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 64.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 287.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 264.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Downloading readme: 100%|██████████| 562/562 [00:00<00:00, 2.68MB/s]\n",
      "Downloading data: 100%|██████████| 905k/905k [00:00<00:00, 1.70MB/s]\n",
      "Downloading data: 100%|██████████| 141k/141k [00:00<00:00, 363kB/s]\n",
      "Downloading data: 100%|██████████| 154k/154k [00:00<00:00, 613kB/s]\n",
      "Generating train split: 100%|██████████| 1027/1027 [00:00<00:00, 8099.65 examples/s]\n",
      "Generating validation split: 100%|██████████| 120/120 [00:00<00:00, 1483.65 examples/s]\n",
      "Generating test split: 100%|██████████| 121/121 [00:00<00:00, 1023.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dir = '../Dataset/Gas'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "window_size = 5\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "\n",
    "        if window_size == 1:\n",
    "            window = \"1 week\"\n",
    "        else:\n",
    "            window = f\"{window_size} weeks\"\n",
    "        example_output = {}\n",
    "        for i in range(window_size):\n",
    "            example_output[f\"week_{i+1+window_size}\"] = {\"time-period\": \"<time-period>\", \"summary\": \"<summary>\"}\n",
    "        # example_output = \"{\\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}, ... \\\"week_n\\\":{\\\"time-period\\\": <time-period>,\\\"summary\\\": <summary>}}\"\n",
    "        example_output = json.dumps(example_output)\n",
    "        instruction = f\"Given the summary and time period for {window},please predict next {window} summary in json format. And ouput only the json data, the response should only contain {example_output}\"\n",
    "\n",
    "        df = summaries[['summary', 'fut_summary']].rename(columns={'summary': 'input', 'fut_summary': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    df = combine_window(df, window_size, \"week\")\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    elif filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    elif filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "        \n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/gas-{window_size}_week-text-text\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(f\"Howard881010/gas-{window_size}_week-text-text\", \"text-text\", f\"{window_size}_week\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
