{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/multimodal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import DatasetDict, Dataset, Features, Value\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the json file to csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "dir = '../Financial_Alpha_Vantage/Summary_with_price_v0.2/'\n",
    "data_dict = {}\n",
    "\n",
    "# Iterate through each JSON file in the directory\n",
    "for company in os.listdir(dir):\n",
    "    json_dir = os.path.join(dir, company)\n",
    "\n",
    "# Get list of JSON files, sorted by filename (assuming they are named in chronological order)\n",
    "    json_files = sorted(glob(os.path.join(json_dir, '*.json')))\n",
    "\n",
    "    # List to store input and output data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through JSON files\n",
    "    for i in range(len(json_files) - 1):\n",
    "        # Read current JSON file\n",
    "        with open(json_files[i], 'r') as current_file:\n",
    "            current_data = json.load(current_file)\n",
    "        \n",
    "        # Read next JSON file for the summary key\n",
    "        with open(json_files[i + 1], 'r') as next_file:\n",
    "            next_data = json.load(next_file)\n",
    "        \n",
    "        # Extract input and output data\n",
    "        input_data = current_data  # Convert JSON to string\n",
    "        input_price = {'shareprice': current_data.get('share_price', '')}\n",
    "        output_summary_price = {'summary': next_data.get('summary', ''), 'shareprice': next_data.get('share_price', '')}  # Get summary from next day's JSON\n",
    "        output_price = {'shareprice': next_data.get('share_price', '')}\n",
    "        \n",
    "        # Append to data list\n",
    "        data.append({\n",
    "            'input': input_data,\n",
    "            'input_price': input_price,\n",
    "            'output_summary_price': output_summary_price,\n",
    "            'output_price': output_price\n",
    "        })\n",
    "\n",
    "    # Convert data list to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(len(df))\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    output_csv_path = f\"../Dataset/Finance/v0.2/{company}.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path, train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate the validation and test sizes\n",
    "    val_size = validation_ratio / (test_ratio + validation_ratio)\n",
    "    \n",
    "    # Split the data into train and temporary datasets\n",
    "    train_data, temp_data = train_test_split(data, test_size=(1 - train_ratio), random_state=42, shuffle=False)\n",
    "    \n",
    "    # Split the temporary dataset into validation and test datasets\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=val_size, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Save the datasets\n",
    "    \n",
    "    dir = file_path.split('/')[:-1]\n",
    "    dir = '/'.join(dir)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    train_data.to_csv(f'{dir}/train_{file_name}', index=False)\n",
    "    validation_data.to_csv(f'{dir}/val_{file_name}', index=False)\n",
    "    test_data.to_csv(f'{dir}/test_{file_name}', index=False)\n",
    "\n",
    "# List of your CSV files\n",
    "dir = '../Dataset/Finance/v0.2'\n",
    "\n",
    "# Loop through each file and split the data\n",
    "for filename in os.listdir(dir):\n",
    "    if not filename.startswith(\"train\") and not filename.startswith(\"test\") and not filename.startswith(\"val\"):\n",
    "        path = os.path.join(dir, filename)\n",
    "        split_data(path)  # Adjust the directory path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(dataframe_test, dataframe_train, dataframe_val):\n",
    "    train = pd.concat(dataframe_train)\n",
    "    test = pd.concat(dataframe_test)\n",
    "    val = pd.concat(dataframe_val)\n",
    "\n",
    "    train_path = '../parquet_dir/train_finance.parquet'\n",
    "    test_path = '../parquet_dir/test_finance.parquet'\n",
    "    val_path = '../parquet_dir/val_finance.parquet'\n",
    "\n",
    "    train.to_parquet(train_path, engine='pyarrow')\n",
    "    test.to_parquet(test_path, engine='pyarrow')\n",
    "    val.to_parquet(val_path, engine='pyarrow')\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset('parquet', data_files=train_path, split='train')\n",
    "    test_dataset = load_dataset('parquet', data_files=test_path, split='train')\n",
    "    val_dataset = load_dataset('parquet', data_files=val_path, split = 'train')\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_huggingface(dataset_path, case):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "# Access the train split\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        train_dataset = dataset[split]\n",
    "\n",
    "        # Convert the dataset to a Pandas DataFrame\n",
    "        df = train_dataset.to_pandas()\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(f\"../Data/Finance/1_day/{case}/{split}_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case1 number -> number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 128 examples [00:00, 2189.17 examples/s]\n",
      "Generating train split: 17 examples [00:00, 241.18 examples/s]\n",
      "Generating train split: 16 examples [00:00, 305.66 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1580.37ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 994.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 775.57ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "Downloading readme: 100%|██████████| 548/548 [00:00<00:00, 2.42MB/s]\n",
      "Downloading data: 100%|██████████| 4.46k/4.46k [00:00<00:00, 15.5kB/s]\n",
      "Downloading data: 100%|██████████| 2.67k/2.67k [00:00<00:00, 11.2kB/s]\n",
      "Downloading data: 100%|██████████| 2.72k/2.72k [00:00<00:00, 12.2kB/s]\n",
      "Generating train split: 100%|██████████| 128/128 [00:00<00:00, 2550.55 examples/s]\n",
      "Generating validation split: 100%|██████████| 16/16 [00:00<00:00, 283.98 examples/s]\n",
      "Generating test split: 100%|██████████| 17/17 [00:00<00:00, 319.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dir = '../Dataset/Finance/v0.2/'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the share price for the current day, please predict the shared price for next day.\"\n",
    "\n",
    "        df = summaries[['input_price', 'output_price']].rename(columns={'input_price': 'input', 'output_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    if filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    if filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "\n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/finance-numerical\", token=token)\n",
    "\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(\"Howard881010/finance-numerical\", \"numerical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 number + text => number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 128 examples [00:00, 1374.80 examples/s]\n",
      "Generating train split: 17 examples [00:00, 300.91 examples/s]\n",
      "Generating train split: 16 examples [00:00, 226.62 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 248.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 857.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 846.31ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Downloading readme: 100%|██████████| 554/554 [00:00<00:00, 1.48MB/s]\n",
      "Downloading data: 100%|██████████| 207k/207k [00:00<00:00, 855kB/s]\n",
      "Downloading data: 100%|██████████| 46.3k/46.3k [00:00<00:00, 218kB/s]\n",
      "Downloading data: 100%|██████████| 29.1k/29.1k [00:00<00:00, 132kB/s]\n",
      "Generating train split: 100%|██████████| 128/128 [00:00<00:00, 1379.93 examples/s]\n",
      "Generating validation split: 100%|██████████| 16/16 [00:00<00:00, 236.15 examples/s]\n",
      "Generating test split: 100%|██████████| 17/17 [00:00<00:00, 258.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dir = '../Dataset/Finance/v0.2/'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the financial report and the share price for the current day, please predict the shared price for next day.\"\n",
    "\n",
    "        df = summaries[['input', 'output_price']].rename(columns={'output_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    if filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    if filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "\n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/finance-mixed-numerical\", token=token)\n",
    "\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(\"Howard881010/finance-mixed-numerical\", \"mixed-numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 number + text => number + summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 128 examples [00:00, 1623.43 examples/s]\n",
      "Generating train split: 17 examples [00:00, 214.80 examples/s]\n",
      "Generating train split: 16 examples [00:00, 231.73 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 164.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 742.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 754.10ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Downloading readme: 100%|██████████| 554/554 [00:00<00:00, 2.69MB/s]\n",
      "Downloading data: 100%|██████████| 250k/250k [00:00<00:00, 1.02MB/s]\n",
      "Downloading data: 100%|██████████| 55.3k/55.3k [00:00<00:00, 225kB/s]\n",
      "Downloading data: 100%|██████████| 36.7k/36.7k [00:00<00:00, 163kB/s]\n",
      "Generating train split: 100%|██████████| 128/128 [00:00<00:00, 1412.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 16/16 [00:00<00:00, 221.78 examples/s]\n",
      "Generating test split: 100%|██████████| 17/17 [00:00<00:00, 243.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dir = '../Dataset/Finance/v0.2/'\n",
    "# case 1: number -> number\n",
    "dataframe_train = []\n",
    "dataframe_test = []\n",
    "dataframe_val = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir):\n",
    "    if filename.startswith('train') or filename.startswith('test') or filename.startswith('val'):\n",
    "        split = filename.split('_')[0]\n",
    "        path = os.path.join(dir, filename)\n",
    "        summaries = pd.read_csv(path)\n",
    "        instruction = \"Given the financial report and share price for the current day, please predict the summary and shared price part for next day.\"\n",
    "\n",
    "        df = summaries[['input', 'output_summary_price']].rename(columns={'output_summary_price': 'output'})\n",
    "        df['instruction'] = instruction\n",
    "        # skip the first and last historical_size days\n",
    "    \n",
    "    if filename.startswith('test'):\n",
    "        dataframe_test.append(df)\n",
    "    if filename.startswith('train'):\n",
    "        dataframe_train.append(df)\n",
    "    if filename.startswith('val'):\n",
    "        dataframe_val.append(df)\n",
    "\n",
    "dataset_dict = convert_to_parquet(dataframe_test, dataframe_train, dataframe_val)\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(f\"Howard881010/finance-mixed-summary\", token=token)\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "load_from_huggingface(\"Howard881010/finance-mixed-summary\", \"mixed-summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmforcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
